<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[小可爱专用 -- 手机照片清理教程]]></title>
    <url>%2F%E5%B0%8F%E5%8F%AF%E7%88%B1%E4%B8%93%E7%94%A8-%E6%89%8B%E6%9C%BA%E7%85%A7%E7%89%87%E6%B8%85%E7%90%86%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[爱你 lallallll 准备阶段 电脑和手机都需下载 360 手机助手 电脑端安装：鼠标点击“360手机助手”这几个字就可以跳转到下载界面，如下图：在上图箭头指向处下载电脑版360手机助手，安装如同其他电脑软件一样，一直确定即可安装完成。 手机端安装：直接在应用商店搜索下载即可。 至于为什么用360手机助手，因为该软件可以将你的手机中的照片很好地按照日期展开，你查看转移时比较方便操作。 电脑通过 360 手机助手 连接手机 首先你需要打开电脑端的 360 手机助手软件 其次如果你要打开 手机上的开发者模式，教程如下： 打开 设置 -》 我的设备 -》 全部参数 -》 MIUI版本，如下图 此时你需要用手指连续点击 上图中的 MIUI版本这几个字，大约连续点击4-5次，每点击一次，该界面底部就会提示你还需点击几次，如下图红色框标注所示，因为我已经打开后了，所以再点击，就会提示：我已经处在开发者模式。 当第 2 步完成后，返回 设置主界面 -》 更多设置 -》 开发者选项 -》点击开启开发者选项 以及USB调试注意：当你点击 USB调试 时，会提示你“USB 调试仅用于开发目的。该功能可用于在您的计算机和设备之间赋值数据，在您的设备上安装应用（事先不发通知）以及读取日志数据。”等弹窗，你直接点击确定既可以。具体操作流程如下图所示： 正式开始备份图片 此时已经将所有的准备工作完成，接下来将手机用数据线与电脑连接，并打开电脑端的360 手机助手软件。此时手机界面可能会弹出一个弹窗：让你确认是否信任该电脑，你点击确定即可。 然后在电脑端的360手机助手界面，应该会自动连接手机了，如果没有自动连接，你可以点击该界面的左上角按钮进行连接，如下图时，即表示你已经连接成功。 然后你可以点击界面中的 照片，视频进行操作了，如下图：如，点击照片按钮，进入以下界面后，可以选中照片，然后点击 导出选中照片 按钮，将照片导出到指定目录，你可以直接将其导入到你的硬盘中，或者电脑的某一目录下。然后这些已经导出备份的照片，你就可以选择性的删除了（点击删除照片按钮），这样你的手机中的剩余空间就会变大了。 结尾 不知道以上操作流程是否能满足您的需求，操作期间有任何问题，请咨询你的小哥哥！爱你哦！]]></content>
      <categories>
        <category>亲爱的一冰专属</category>
      </categories>
      <tags>
        <tag>小可爱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二篇 -- 如何配置在]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E5%BA%93%2F</url>
    <content type="text"><![CDATA[本文是C++调用mask rcnn训练的模型的系列文章中的第二篇，主要讲述了如何配置C++端调用tensorflow API所需要的库，包括了 编译TensorFlow1.12 C++ API 以及 Eigen、protobuf等库 &lt;!— more — 环境说明 因为最终调用 pb文件 是在 win10的VS2015或17中实现的，所以编译所需库的配置环境如下： Win10 带有GPU的，我试过的有 NVIDIA GeForce GTX 1060、1050 配置相应的CUDA以及Cudnn Visual Studio 2015或2017 背景说明 网上有很多用Bazel和CMake编译的教程，教你如何去配置，但是我看了很多教程都没有成功。 最终我采用了github一大神写的脚本，自动的帮你编译TensorFlow以及所需的其他库，他用的也是Bazel编译，只不过是帮你封装好了，你直接运行该脚本，就可以按照他的提示进行，最后成功编译 实现首先看一下大神该脚本的github主页：https://github.com/guikarist/tensorflow-windows-build-script 首先你要仔细的看一下这个项目的说明 Getting StratedPrerequisites在开始之前，你要先准备以下： 如果您需要构建GPU版本，则需要按照此官方指南安装GPU支持。即安装CUDA以及Cudnn]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>实力分割</tag>
        <tag>mask rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇 -- How to convert Mask RCNN model to Tensorflow .pb]]></title>
    <url>%2FHow%20to%20convert%20Mask%20Rcnn%20model%20to%20Tensorflow%20.pb%2F</url>
    <content type="text"><![CDATA[本文是C++调用mask rcnn训练的模型的系列文章中的第一篇，主要讲述了为了在C++中能够调用训练的.pb模型，就必须将训练得到的.h5模型格式转化为pb格式。 背景 由何凯明大神团队提出的 Mask_RCNN 一经发布就受到了广大的关注和喜爱，该模型可以实现实力分割和目标检测等功能。因为我在做一个实力分割的项目，最终需要部署到 window 上的 C++ 的环境下，所以就开始了这漫长又艰苦的历程。 我们知道由大神github大神写了一个基于 Keras 写的 maskrcnn的源码，写的很好。我也是用这个版本进行训练我的模型的，因为是基于Keras写的源码，所以最后训练得到的模型格式是 .h5。 Pipline由于我们最终需要在C++中部署，调用该模型，所以我们的整体流程是： Keras训练 -&gt; my_model.h5 -&gt; 转换为pb类型 -&gt; my_model.pb -&gt; C++端用tensorflow API调用并运行模型 必备环境配置 将 .h5 文件转为 .pb 文件，我的环境配置如下： Ubuntu16.04 GTX1080 用anaconda创建一个mask rcnn的环境（实现时是在该环境下输入的命令） mask rcnn环境配置需要安装以下库(最好用conda命令安装，没有的再用pip安装)： 123456789101112numpyscipyPillowcythonmatplotlibscikit-imagetensorflow&gt;=1.3.0keras&gt;=2.0.8opencv-pythonh5pyimgaugIPython[all] 实现不得不说github是个好东西，某大神写了一个脚本，一条命令就可以将keras 训练得到的 .h5 模型转为对应的 .pb 模型 文件下载地址 打开链接后，发现scripts文件夹下有三个 .py 文件，我们用到的就一个 export.py 文件，我们可以将整个 scripts 文件夹下载下来，放到如该作者放置的目录下（即mask rcnn源码根目录下） 然后进入scripts文件下，在mask rcnn的环境下，输入以下命令 1python export.py --model_path=../logs/partsorting20181219T2157/mask_rcnn_partsorting_0399.h5 --log_dirpath=../logs/ --config_path=../mrcnn/config.py 在export.py 文件中已经详细的解释了 —model_path、—log_dirpath、—config_path这三个参数的具体含义，所以很好理解。运行完这条命令后，即可得到 对应的 .pb 文件。 总结有什么操作步骤不太理解，可以随时联系我，我看到后会尽快回复！]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>实力分割</tag>
        <tag>mask rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNet V2]]></title>
    <url>%2FMobileNetV2-%20Inverted%20Residuals%20and%20Linear%20Bottlenecks%2F</url>
    <content type="text"><![CDATA[论文动机 神经网络在机器智能的很多领域都有革命性的改进，在图像识别的领域精确度已经超过人类。然而，为了提高精确度尝尝会带来消耗，需要更高的计算资源，是很多手机和嵌入式设备所不具有的。这篇文章介绍了一个新的神经网络结构，是专门为手机和资源有限的环境量身定制的。该网络通过减少计算次数和内存占用，推进了为移动设备量身定制的计算机视觉模型达到了一个更先进的水平。 本文是Google团队在MobileNet基础上提出的MobileNet V2，实现分类、目标检测、语义分割多目标任务 主要贡献是具有线性瓶颈的倒置残差。 该模块将低维压缩的输入，首先扩展到高维，并使用轻量级深度卷积进行过滤。 随后通过线性卷积将特征投影回低维表示。 Preliminaries, discussion and intuitionDepthwise Separable Convolutions Depthwise Separable Convolutions已经在MobileNet V1 中讲过，其基本思想：将标准卷积差分为两个卷积：深度卷积（depthwise convolution）和逐点卷积（pointwise convolution）。 深度卷积：对每个输入通道应用单通道的轻量级滤波器（即，深度卷积后，通道数不变，是单独分别对每个通道进行卷积） 逐点卷积：负责计算通道的线性组合构建新的特征（即此时用 1*1 的卷积核对深度卷积后的所有的通道进行卷积操作，不再是单独分别卷积了） 标准卷积 $L_i$ 为 $h_i w_i d_i$，应用 $kk$ 的卷积核产生输出 $L_j$ 为 $h_i w_i * d_j $ 标准卷积的计算消耗为：$h_i w_i d_j k k * d_i$ 拆分后深度分离卷积计算消耗为：$h_i w_i d_j d_i + h_i w_i d_i k k = h_i w_i * d_i (k^2 +d_j)$ 计算消耗比例因子为：$\frac{k^2d_j}{k^2+d_j}$ MobileNet V2 中使用的卷积核大小 k = 3，与标准卷积相比计算量减少了 8~9倍，精读上有略微的损失。 注：对于计算消耗如何算出来的，可以从卷积得到后feature map大小进行计算。比如标准卷积，卷积后的feature map 为：$L_j$ 为 $h_i w_i d_j $ ，所以 卷积后的feature map 有 $h_i w_i d_j$ 个点，每个点是由 $k k d_i$ 次计算得到的，所以标准卷积的计算消耗为：$h_i w_i d_j k k * d_i$ Linear Bottlenecks这部分论文中的理论很难理解，我先说我个人看完论文理论部分和网上博客讲解后的个人见解 个人见解当channel 的个数比较少的时候，所有的信息都集中在比较窄的channel中，这时候进行非线性激活函数ReLU，会丢失很多信息。而在MobileNet V1中引入了一个超参数 width multiplier会缩减channel，这样看起来就像一个瓶子的颈部一样，这个时候用ReLU激活函数会丢失掉不少的信息，造成精度低。 看一张论文中关于 ReLU 变换导致丢失信息的的图解： 注：input 是一张2维数据，其中manifold of interest是蓝色的螺纹线；使用随机矩阵 T 将数据嵌入到 n 维空间汇总，后接ReLU，再使用$T^{-1}$将其投影回2D平面 由上图可以看出，n=2、3时信息丢失严重，中心点坍塌。当n=15……30之间，恢复的信息明显增多了。 所以我们经过上图可以得出，当channel越小，丢失的信息越多，当channel越大，丢失的信息越少。其实不难理解。如果channel为2时，信息都集中在这两个channel中，如果有部分数值小于0就会被ReLU激活丢失掉。如果channel 为30 其实信息是分散的，而且具有冗余，所以通过ReLU激活后归于0的值可能在其他的通道中还存在，便不会影响太多信息的存储。 所以作者建议对于channel数很少的那些层（即低维空间时）做线性激活代替原本的ReLU非线性激活。所以优化网络架构的思路就出来了：通过在卷及模块后插入linear bottlenecks来捕获感兴趣特征实验证明，使用linear bottlenecks可以防止非线性破坏太多信息。 bottleneck就表示缩减的层，linear bottleneck表示对channel缩减的层做线性激活。如果要用ReLU激活就需要先增加channel数再做ReLU。 论文中两个结论这里我直记录文章中的两个结论。至于论文中其余的理论讲解，我很难讲述出来，但是上面的个人见解我认为已经将这一结构讲述清楚了，如果直接看论文，恐怕会被理论搞糊涂。 1. If the **manifold of interest** remains non-zero volume after ReLU transformation, it corresponds to a linear transformation. (感兴趣区域在ReLU之后保持非零的区域，近似认为是线性变换。可以从ReLU的曲线中得到！) 2. ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. （ReLU能够保持输入信息的完整性，但仅限于输入特征位于输入空间的**低维子空间中**） 首先对于结论1中的manifold of interest概念说一下： manifold of interest：作者将 $whc$ 的一个feature map 理解特征图上的一个点有 c维特征！而manifold of interest 表示对于我们分类或者识别任务有用的特征，显然要提高分类识别精度，则 manifold of interest 需要分布在这c维特征空间或其子空间中！，这就是 manifold of interest索要表到的意思。 bottleneck convolution 的基本实现 根据上面个人理解和论文中的两个结论，我们可以就可以知道如何搭建这个网络了。 首先借助结论2，假设输入特征图中包含了分类识别所需的所有特征分布（manifold of interest）在输入的c个维度中。因此我们首先将输入维度进行拓展，拓展成 t*c个维度（t&gt;1,论文中为6）。因此此时manifold of interest分布在 t*c的子空间中，这时候，根据前面的推论，高纬度时，ReLU不会损失太多信息，所以可以采用ReLU激活函数保留输入的特征，而且ReLU还可以增加一些复杂性可以更好地提升性能。 然后借助结论1，通过ReLU激活函数之后，特征的维度依然是t*c，这时候缩小其特征维度变成 c’,相当于对于manifold of interest进行了维度压缩，当然这些特征都是非0的，因此，按照结论1，ReLU对非0的特征相当于做了一个线性的映射，因此，就可以去掉ReLU激活函数。 所以根据以上两个步骤，我们就可以理解bottleneck的结构，如下表，由如下的三个部分构成：： 首先是1*1 conv2d变换通道，后接ReLU6激活（ReLU6即最高输出为6，超过了会clip下来） 中间是深度卷积，后接ReLU 最后是1*1 conv2d后面不接ReLU了，而是论文中提出的linear bottleneck 注：论文中扩张系数 t 为6 Inverted residuals a. 普通结果使用标准卷积将空间合同到信息一起映射到下一层，参数和计算量会比较大 b. MobileNet V1中使用的深度可分离卷积 c和d图是MobileNet V2的结构（c、d图其实是从模型不同起点观看形成的图，d是c的下一个连接状态） 下图是普通的residual block 与 inverted residual block之间的对比 每个长方体的厚度代表通道数 a为传统的residual block。结构为： 1 1卷积（降维）+ ReLU -&gt; 3 3卷积 + ReLU -&gt; 1 * 1卷积（升维）+ ReLU b为倒置的residual block（inverted residual block）。结构为：1 1卷积（升维）+ ReLU -&gt; 3 3depthwise separable卷积 + ReLU -&gt; 1 * 1卷积（降维）（注意这里是不带ReLU的，即前面提到的linear bottleneck）。 为什么用 inverted residual block 而不直接用residual block？ 至于为什么这么做，我们可以尝试理解一下 我们知道 MobileNet V1 最大特点就是采用了depthwise separable convolution来减少运算量以及参数量，而在网络结构中没有采用shortcut（shortcut实现的方式就是residual block，其实两者算是一回事）的方式。 但是我们也知道ResNet以及Densenet等一系列采用shortcut的网络的成功，表明这个shortcut是个非常好的东西，于是MobileNet V2就将这个结构拿来用。 但是也不能直接拿来用啊，毕竟要适应自身的特点（depthwise separable convolution），如果直接把depthwise separable convolution应用到 residual block中，会碰到下面两个问题： DWConv layer层提取得到的特征受限于输入的通道数，若是采用以往的residual block，先“压缩”，再卷积提特征，那么DWConv layer可提取得特征就太少了，因此一开始不“压缩”，MobileNetV2反其道而行，一开始先“扩张”，本文实验“扩张”倍数为6。 通常residual block里面是 “压缩”→“卷积提特征”→“扩张”，MobileNetV2就变成了 “扩张”→“卷积提特征”→ “压缩”，因此称为Inverted residuals 当采用“扩张”→“卷积提特征”→ “压缩”时，在“压缩”之后会碰到一个问题，那就是Relu会破坏特征。为什么这里的Relu会破坏特征呢？这得从Relu的性质说起，Relu对于负的输入，输出全为零；而本来特征就已经被“压缩”，再经过Relu的话，又要“损失”一部分特征，因此这里不采用Relu，实验结果表明这样做是正确的，这就称为Linear bottlenecks 模型结构MobileNet V2其中：t表示“扩张”倍数，c表示输出通道数，n表示重复次数，s表示步长stride。 先说两点有误之处吧： 第五行，也就是第7~10个bottleneck，stride=2，分辨率应该从28降低到14；如果不是分辨率出错，那就应该是stride=1； 文中提到共计采用19个bottleneck，但是这里只有17个。 注：文中已经说明n表示重复次数，比如表中第4行，n=3，stride=2，不是说重复的这三次stride都为2，只有第一次操作时，stride为2，后面重复的两次stride都为1 特别的，针对 stride=1 和 stride=2 ，在block上有稍微的不同，主要是为了与shortcut的维度匹配。因此，stride=2时，不采用shortcut结构，具体如下图： MobileNet V1、MobileNet V2和ResNet微结构对比可以看到 MobileNetV2 和 ResNet 基本结构很相似。不过 ResNet 是先降维（0.25 倍）、提特征、再升维。而 MobileNetV2 则是先升维（6 倍）、提特征、再降维。 实验结果ImageNet Classification 在 ImageNet 数据集对比了 MobileNetV1、ShuffleNet，MobileNetV2 三个模型的 Top1 精度，Params 和 CPU（Google Pixel 1 phone）执行时间。MobileNetV2 运行时间 143ms，参数 6.9M，Top1 精度 74.7 Object Detectio 论文以 MobileNetV2 为基本分类网络，实现 MNet V2 + SSDLite，耗时 200ms，mAP 22.1，参数 只有 4.3M。相比之下，YOLOv2 mAP 21.6，参数50.7M。模型的精度比 SSD300 和 SSD512 略低 Semantic Segmentation 当前 Semantic Segmentation 性能最高的架构是 DeepLabv3，论文在 MobileNetV2 基础上实现 DeepLabv3，同时与基于 ResNet-101 的架构做对比，实验效果显示 MNet V2 mIOU 75.32，参数 2.11M，而 ResNet-101 mIOU80.49，参数 58.16M，明显 MNet V2 在实时性方面具有优势。 结论CNN 在 CV 领域突破不断，但是在小型化性能方面却差强人意。目前 MobileNet、ShuffleNet 参数个位数（单位 M）在 ImageNet 数据集，依 top-1 而论，比 ResNet-34，VGG19 精度高，比 ResNet-50 精度低。实时性和精度是一对欢喜冤家。 本文最难理解的其实是 Linear Bottlenecks，论文中用很多公式表达这个思想，但是实现上非常简单，就是在 MobileNetV2 微结构中第二个 PW 后无 ReLU6。对于低维空间而言，进行线性映射会保存特征，而非线性映射会破坏特征。 参考链接 https://blog.csdn.net/stesha_chen/article/details/82744320 https://blog.csdn.net/u011974639/article/details/79199588 https://blog.csdn.net/c9yv2cf9i06k2a9e/article/details/79237932 https://blog.csdn.net/hongbin_xu/article/details/82992589 https://blog.csdn.net/u014380165/article/details/79200958 https://blog.csdn.net/Charel_CHEN/article/details/81281778?utm_source=blogxgwz0 https://blog.csdn.net/u011995719/article/details/79135818]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度卷积可分离参数计算]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[基本介绍 第一次在“MobileNet V1”论文中看到“深度可分离卷积（Depthwize Separable Convolution）”。其实该论文重点也是在讲这个结构。现在讲述该概念。 深度可分离卷积是一种将标准卷机分解成深度卷积和一个1*1的卷积（即逐点卷积）。 深度卷积对于MobileNet而言，对每个单个输入通道应用单个滤波器进行滤波，然后逐点卷积应用1*1卷积操作来结合所有深度卷积得到的输出。 标准卷积一部即对所有的输入进行结合得到新的一系列输出。深度可分离卷积将其分成两步，针对每个单独层进行滤波，然后下一步即结合。 这种分解能够有效地大量减少计算量以及模型的大小。 参数计算 假设某一层输入通道是8，输出通道是16，使用的卷积核是3x3，使用正常卷积那么这层的参数计算方式为（8x3x3+1）x16=1168，其中1是偏置带来的参数。具体为，输入通道数据被8个不同3×3大小的卷积核会遍历16次，从而产生8×16=128个特征图谱，进而通过叠加8个输入通道对应的特征图谱后融合得到1个特征图谱，最后可得到所需的16个输出通道。 在可分离卷积里面参数计算方式为8x3x3+16x(8x1x1+1)=216,其中和8相乘的1是大小为1卷积核参数，最后加的1是偏置带来的参数。具体为，输入通道数据被8个不同3x3大小的卷积核遍历1次，生成8个特征图谱，8个特征图谱中每个被16个1x1卷积核遍历，生成8x16=128个特征图谱，进而通过叠加8个输入通道对应的特征图谱后融合得到1个特征图谱，最后可得到所需的32个输出通道。 通过分离卷积操作参数从1168个降到216个，可见在模型复制度上有很大的优化。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLab V1]]></title>
    <url>%2FDeepLab%20V1%2F</url>
    <content type="text"><![CDATA[背景 尽管之前的一些文章中提出的方法（如FCN、SegNet、U-Net）实验效果不错，但是还是无法避免一些问题，比如，精度问题、对细节不敏感、以及像素与像素之间的关系没有考虑、忽略空间的一致性等问题。 于是更牛的人提出了一种新的卷积计算方式开始称为“带hole”的卷积，也就是使用一种“稀疏的卷积核”来计算，以此取代池化的处理。 之前我们已经讲过，池化操作能够家少计算量，同时也能防止计算结果过拟合，那么单纯的取消池化操作又会使单层网络的感受域缩小，但是如果使用“稀疏的卷积核”来处理卷积，那么可以达到在不增加计算量的情况下增加感受域，弥补不进行池化处理后的精度问题。 后面这种带洞的卷积方式起了一个高雅的名字，叫做“Dilated Convolutions” Dilated Convolutions解决了减少池化层导致的精度问题，并且这种方法会比之前的方法增加了边界刻画的精度。但是还有第二个问题，即像素与像素之间的逻辑关系问题，毕竟前面再牛叉的算法也只是单纯的计算没有根据无力意义进行判断输出的标注里这些结果是否合法（符合现实逻辑）。 于是很多深度学习框架的图像语义分割系统都使用了一种叫做“条件随机场（）Conditional Random Field”，简称CRF，的技术作为输出结果的优化后处理手段。 所以改文章就结合了CNN与CRF，CNN在high level属性上的预测效果很好，CRF可以带来更高的定位精度，然后提出并采用了hole algorithm（空洞卷积），加快计算速度和提高了精度。 deeplab 结构1. Dense spatial score evalution 把VGG-16 的全连接层改为卷积层 DeepLab的主体结构事实上是参照VGG改造的，首先就是他把VGG-16的全连接层改为了卷积层，整个网络相当于变为了一个全卷积网络。 问题在于原本的VGG-16有5个pooling层，这样的话在pooling5输出的feature就非常的稀疏，因为他把原有尺寸缩小了32倍（2^5，即整体的stride=32）。这恰恰是全卷积网络的通病，因为缩小的太厉害了，在还原图像大小时就会不那么dense。所以为了得到更dense的图像，有了下面的操作 为了让获得的图像更加dense，将pooling4余pooling5的stride由2变为1 为了更加dense，将pooling4、5的stride由2变为1，那也就是说pooling4、5层不会缩小图像了，所以只有前三层的pooling会导致图像缩小，也就是最终将图像缩小了8倍，如下图： 这样的话虽然，更加dense，但是尺寸缩小为原来的8倍，之后的节点的感受野就会发生变化，因此想要利用原先的VGG16的weights进行微调就有问题。 这儿有个感受野的计算公式： 12345RF = 1 # 待计算的feature map上的感受野的大小，初始化为1. for layer in (top layer To down layer)： RF = ((RF-1)*stride) + fsize # stride 表示卷积的步长；fsize表示卷积层滤波器的大小 感受野需要从当前层往前推，也就是“当前这一层的节点往前看能看到多少前几层的节点”。公式中假设当前层的RF=1，stride是上一层的步长，padding不影响感受野大小，这是一个递推公式，为了更好的理解，请看下图： 最左边的图就是VGG原来的结构，可以看到最下方的层往前看，能看到4个最上面层的节点，标号从左到右分别是{1,2,3,4}，{3,4,5,6}，{5,6,7,8}，右边是将pooling的stride变为1之后的图，显而易见，最下方的节点的RF减少，只能看到最上面层的三个节点。 为了解决这个将pooling4、5的stride缩小后导致的感受野问题，作者提出了Hole算法 Hole算法 为了保证感受野不发生变化，某一层的stride由2变为1后，后面的层需要采用hole 算法，具体的来讲就是将连续的连接关系根据 hole size大小变为skip连接的。 pooling4的stride 由2 变为1，则紧接着的conv5_1,conv5_2和conv5_3中hole size为2. 接着pool5由2变为1, 则后面的fc6中hole size为4。 同时由于Hole算法让feature map更加dense(因为最后VGG输出的是下采样8倍，所以high level feature map 变化十分平滑)，所以网络直接用**双线性插值**将feature map上采样到和原图相同的大小就能获得很好的结果，这个步骤还不用去学习上采样的参数了，几乎没有计算损耗。FCN等全卷积网络因为采用了de-convolution，没有采用Hole卷积，因此想要达到相同的感受野，需要下采样到32倍，而想要恢复到原图大小，需要对上采样的参数进行学习（否则精度无法达到），因此DeepLab的CNN部分仅需要10h的训练时间，而其他的FCN可能需要几天的时间 是用了Hole 卷积，网络结构变为： 要解释为什么hole卷积会使感受野变大，还是得回到刚才那个简单的问题，如下图： 第一层8个，第二层经过stride=1的pooling层后，就会减一变为7，然后进行Hole卷积，后我们可以看到感受野没有变化，都能看到最上层的4个神经元。运用hole算法之后实际的 kernel_size有个计算公式：$ke = k+(k-1)(r-1)$ ,这里的r是rate，就是 input stride， 其实就是相当于将卷积核填0扩充，如下图： 所以这就解决了由于只缩小了8倍导致的感受野的问题。 对VGG16进行finetune 将原来分类的1000类改为21类，因为要分割出21种物体，同时把损失函数改为交叉熵的形式。即对于样本集，p为真实分布，q为预测分布，则有： H(p) = \sum_i{p(i) * log \frac{1}{p(i)}}这和逻辑回归的损失函数是一样，只是将2类拓展到多类。 2. Controlling the network‘s respective field size 对FC6的卷积核直接降采样 因为之前的Fc layer种，计算量相对较小（VGG第一个fc layer为1*1*4096），而改为fully convolutional layer之后，大小变成了 7*7*4096，这大大增加了计算量核存储空间，因此将其size进行缩减，变成4*4或者3*3，减少参数量。（直接从7x7的范围中抽取4x4或者3x3的范围以减少计算量，计算时间比原来减少了2-3倍） VGG16的感受野是224*224，把网络改为全卷积的话是404*404（fc6的卷积核大小为7*7）。改为4*4时感受野变味了308 把FC6输出的feature map 从4096 减少到1024 将feature channel 数量减少来减少模型的尺寸 以上两个步骤都加快了运算速度。 上图中的降采样就是将7x7卷积核缩小为4x4或3x3 细节边界恢复 细节边界恢复：完全连接的条件随机场和多尺寸预测。 在上采样之后加入完全连接的CRF（条件随机场）精细化边缘 越深的网络分类越精确，但是因为不变性核大感受野导致定位不精确。解决这个问题有两种主要的方法： 将low level和high level的feature map 进行融合，FCN就是这样做的。 引入super-pixel representation，用low level segmentation method来进行定位任务。 论文中使用全连接的条件随机场（CRF）方法来对定位做finetune，这比当前的方法都要好。 CRF之前就被用于平滑包含大量噪声的结果的工作中（不是全连接的）。整个模型的pipeline如下： 下面图中第一行是在输入softmax之前做CRF得到的结果，下面一行是softmax的输出经过CRF得到的结果 简单的介绍一下CRF： 我们用下面的公式来表示输出结果的整体能量，或者说混沌程度，称为能量函数（energy function）： E(x)=\sum_i \theta_i(x_i) + \sum_{ij} \theta_{ij}(x_i,x_j).前一项戴白哦像素的内聚程度，其中，P($x_i$)就是DCNN输出的score map 在这i这歌像素上，它的真实标签的概率。 后一项戴表相邻节点的相关程度。 w_1 exp(- \frac{||p_i-p_j||^2}{2 \sigma_\alpha^2} - \frac{||I_i - I_j||^2}{2 \sigma_\beta^2}) + w_2 exp(- \frac{||p_i-p_j||^2}{2 \sigma_\gamma ^2})其中二元能量项描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。二元势函数描述的是每一个像素语其他所有像素的关系，所以叫“全连接” 对每一个类（现在有21类）求解E(x)，当取到 minE(x)时像素值最稳定。 Multi-scale prediction 使用多尺度预测，提供更加准确的分割结果：将输入图像通过2层的感知机，与前四层的pooling layer输出进行concatenate，再输入到softmax激活函数中，相当于softmax的输入channel是640。 这是参考FCN的做法，将INPUT，pooling1，pooling2，pooling3，pooling4的输出结果信息传到sofftmax，因为随着层数的加深，一些细节还是会被丢失。具体的做法是在这几个地方加入多层感知机，其实就是3*3*128和1*1*128两个卷积层，最后再融合到softmax。 实验对比 评测结果如下图，可以看出是直接秒杀当时其他 state-of-art的方法。 下图是检验便捷分割的准确率，在边界设置一个窄带（白色部分），叫做trimap，在trimap中计算mean IoU，然后加大trimap的宽度，再计算准确率，得到曲线。 下图是DeepLab在PASCAL VOC2012的测试集上的准确率对比 结论 论文最主要的点就是结合了CNN与CRF，在保证高分类精度的前提下，也保证了很高的定位精度。 引入孔洞卷积，在不要降采样或者增大kernel的情况下就可以增大feature map的感受野。 使用多尺度的方法进一步提升分割准确度。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android studio 如何配置OpenCV的]]></title>
    <url>%2Fandroid-studio-%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEOpenCV%E7%9A%84%2F</url>
    <content type="text"><![CDATA[讲解了如何在Android studio中配置opencv android studio 如何配置OpenCV的前期准备工作，你需要在OpenCV官网下载指定的安装包，是OpenCV for Android类型的，如下图 下载完后解压后的目录如下图： 只写java版的OpenCV环境配置现在将OpenCV引进android studio中 1、新建一个android 工程 2、点击File—&gt; New —&gt; Import Module 如下图 3、在第2步后，会弹出如下窗口 4、找到OpenCV解压的路径，选择sdk/java文件夹，如下图 5、之后直接点击Next —&gt; Finish。然后右击该项目—&gt; Open Module Settings —&gt; Dependencies —&gt; 点击右边的 加号 —&gt; Module dependency —&gt; 选择openCVLibrary310 —&gt; 点击 OK。如下图 6、此时项目可能报错，此时我们可以更新build.gradle文件。这是因为opencv-Android的在这里的bulid文件和项目中的不一样,将划红线部分的信息改为和app文件夹下的build.gradle中信息相一致，主要是修改以下四个参数的值：compileSdkVersion、buildToolsVersion、minSdkVersion、targetSdkVersion。如下图 7、复制libs文件夹中。在OpenCV的解压包中，将sdk/native/libs文件夹复制，粘贴到 Project视图下的app —&gt; libs 下，然后双击打开 app —&gt; build.gradle 文件，在 android 模块中添加以下代码： 123456sourceSets &#123; main &#123; jniLibs.srcDirs = [&apos;libs&apos;] java.srcDirs = [&apos;src/main/java&apos;, &apos;src/main/java-gen&apos;] &#125; &#125; 要注意括号匹配，是填yunyong写在android模块下，而不是buildTypes模块下。填写完该代码后，该文件的右上方会有一个 Sync Now，点击就Ok了，此时返回android 视图，会发现多出一个jniLibs文件。 到此，第一种配置就完成了。当然可能大家对第7步有异议，其实也可以如下操作：在OpenCV的解压包中，将sdk—&gt;native—&gt;libs文件夹复制，粘贴在Project视图下app—&gt;src—&gt;main目录下，并将其重命名为jniLibs。这种作法等同于我之前讲的。 C++代码中配置OpenCV环境因为有时我们可能需要在android中编写C++代码，有时会在C++代码中调用OpenCV的库，这时候我们应该怎么配置呢？因为android中编写C++代码是需要NDK的支持，其实自从android studio出了2.2版本后直接可以在创建工程时添加NDK支持了，添加之后main文件夹下会多出一个native-lib.cpp这个文件，如果只是为了一个简单的NDK接口，貌似这样就结束了，直接在该cpp文件下编程即可。 但是，如果使用第三方库，就得重新配置了。到目前为止，网上大半文章都是.mk的配置方法。android studio 2.2使用Cmake作为跨平台编译工具，创建完一个工程后，JNI的配置都是由Cmake来管理，在project文件夹下的CmakeList文件中。Cmake有自己的编写规则，最好去官网看一下语法。下面就开始C++配置OpenCV环境，你只需要在CmakeList文件中添加上这几句话就可以了。 12345include_directories(/home/fxr/Android/OpenCV-android-sdk/sdk/native/jni/include)add_library(lib_opencv SHARED IMPORTED)set_target_properties(lib_opencv PROPERTIES IMPORTED_LOCATION /home/fxr/Android/OpenCV-android-sdk/sdk/native/libs/$&#123;ANDROID_ABI&#125;/libopencv_java3.so) 将之前的target_link_libraties模块换成以下这个 1234567target_link_libraries( # Specifies the target library. native-lib # Links the target library to the log library # included in the NDK. $&#123;log-lib&#125; lib_opencv ) 这样应该就可以在C++文件中调用OpenCV的库了。 注意：将其中的路径换成你自己的 上面添加的几句话可以做如下解释： 1 #添加ffmpeg对应的头文件目录,```$&#123;pathToFFMPEG&#125;```为前面配置过的路径,可以替换为```include_directories(E:/ffmpeg/include)```这种路径格式1234567891011121314&gt;&gt; 2 ```add_library( ffmpeg SHARED IMPORTED)``` #添加库文件，实际上就是引入so文件,IMPORT代表从第三方引入的意思&gt; &gt; 3 ```set_target_properties( ffmpeg PROPERTIES IMPORTED_LOCATION $&#123;pathToProject&#125;/app/src/main/jniLibs/$&#123;ANDROID_ABI&#125;/libffmpeg.so```#这句话是ffmpeg对应的so文件,so文件是放到JNILibs这个文件夹中&gt; &gt; 4 ```target_link_libraries( $\&#123;log-lib&#125; native-lib ffmpeg)```#为native-lib加载ffmpeg库.这样配置好就可以在native-lib里调用ffmpeg模块了##### 附加如果是自己编写的CPP文件，只需做如下变化 add_library(coreAlg SHARED src/main/cpp/coreAlg.cpp )#添加库文件,这种实际上是有AS给编译成so文件了target_link_libraries(native-lib ${log-lib} coreAlg)#native-lib里调用coreAlg``` 参考： http://www.jianshu.com/p/7f0cc055c05f http://www.cnblogs.com/meadow-glog/p/6130568.html]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>Android studio</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac安装opencv3并在pycharm中运用]]></title>
    <url>%2FMac%E5%AE%89%E8%A3%85opencv3%E5%B9%B6%E5%9C%A8python%E4%B8%AD%E8%BF%90%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文主要讲解了Mac中安装opencv3，并探索在python编译软件pycharm中如何使用opencv 问题描述环境 Mac pycharm professional 2017.2 问题 pycharm找不到12345678``` - pythonimport cv2img = cv2.imread(&quot;1.jpg&quot;)cv2.namedWindow(&quot;Image&quot;)cv2.imshow(&quot;Image&quot;, img)cv2.waitKey(0)cv2.destroyAllWindows() 解决办法安装opencv如果你还没有安装opencv，那么就要先安装opencv，只需要检查相关的文件夹是否有即可，或者在C++中调用以下opencv，如果能测试成功，即说明安装了opencv。如果没有安装，安装也很简单，因为我的是Mac，所以只需要以下一条命令即可： 1brew install opencv3 这样执行后，安装的便是opencv3.X系列。 配置pycharm安装opencv3成功后，用命令行进入以下目录， 1cd /usr/local/lib/python3.6/site-packages/ 然后在命令行中，输入以下命令： 1ln -s /usr/local/Cellar/opencv/3.4.1_2/lib/python3.6/site-packages/cv2.cpython-36m-darwin.so 这两条命令，你要根据你自己的对应的目录进行相应的更改。 执行完，你发现在/usr/local/lib/python3.6/site-packages/的目录下，在命令行中进入python3，发现调用cv2模块，能成功。而在其他目录下，引用cv模块，而不能成功。因为第二条命令，所以你在/usr/local/lib/python3.6/site-packages/目录下可以import cv2而不会出错。 所以，我们要在pycharm中把该路径加入到环境变量中。具体步骤如下： 首先打开pycharm，点击“Edit Configurations”，如下图： 打开“Edit Configurations”后，需要添加环境变量。点击“Environment variables”即红色箭头所指的按钮 然后选择左下角的“加号”，添加环境变量，Name为“PYTHONPATH”，Value为“/usr/local/lib/python3.6/site-packages/”，此处Value需要改成你对应的目录。如下图： 此时依次点击OK，Apply，OK按钮关掉窗口，再次在pycharm中写下import cv2模块后，就可以随意的在python中使用opencv了。 注意：有时虽然你按照步骤在pycharm中配置好了opencv，在写下import cv2，pycharm也会提示错误，但是不用管它，直接用就好了，不妨碍你使用，如下图： 总结有任何问题，欢迎留言或者直接联系我！]]></content>
      <categories>
        <category>配置</category>
      </categories>
      <tags>
        <tag>opencv3</tag>
        <tag>python</tag>
        <tag>pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树分类算法]]></title>
    <url>%2F%E5%86%B3%E7%AD%96%E6%A0%91%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[概念讲解分类与聚类 在具体讲解分类和聚类算法之前，我们先了解一下什么是分类，什么是聚类，以及它们常用的算法有哪些。 1. Classification(分类)，对于一个classifier，通常需要你告诉它这个东西被分为某某类这样一些例子，理想情况下，一个classifier会从它得到的训练集中进行“学习”，从而具备对未知数据进行分类的能力，这种提供数据训练的过程一般叫做“Supervised Learning（监督学习）”。 2. Clustering（聚类），简单的说就是把相似的东西分到一组，聚类的时候，我们不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。因此一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此clustering通常不需要使用训练数据进行学习，这种叫做“Unsupervised Learning（无监督学习）” 分类：简单地说，就是根据文本的特征或属性，划分到已有的类别中。也就是说，这些类别是已知的，通过对已知分类的数据进行训练和学习，找到这些不同类的特征，再对未分类的数据进行分类。常用算法：决策树、朴素贝叶斯，支持向量机，神经网络，k-最近邻，模糊分类 聚类：简单地说，就是你压根不知道数据会分为几类，通过聚类分析将数据或者说用户聚合成几个群体，那就是聚类了。聚类不需要对数据进行训练和学习，一个聚类算法只要知道如何计算相似度就可以工作了。 什么是决策树所谓决策树，就是一种树，一种依托于策略抉择而建立的树。 机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 从数据产生决策树的机器学习技术叫做决策树学习, 通俗点说就是决策树，说白了，这是一种依托于分类、训练上的预测树，根据已知预测、归类未来。 来理论的太过抽象，下面举一个浅显易懂的例子： 套用俗语，决策树分类的思想类似于找对象。现想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话： 女儿：多大年纪了？ 母亲：26。 女儿：长的帅不帅？ 母亲：挺帅的。 女儿：收入高不？ 母亲：不算很高，中等情况。 女儿：是公务员不？ 母亲：是，在税务局上班呢。 女儿：那好，我去见见。 这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑： 决策树是一种十分常用的分类方法，需要监管学习，监管学习就是给出一堆样本，每个样本都有一组属性和一个分类结果，也就是分类结果已知，那么通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。 相关算法决策树学习之ID3算法从信息论知识中我们知道期望信息越小，信息增益越大，从而纯度越高。ID3的算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益（后面会讲解什么是信息增益）最大的属性进行分裂 所以ID3的算法思想便是： 自顶向下的贪婪搜索遍历可能的决策树空间构造决策树(此方法是ID3算法和C4.5算法的基础)； 从“哪一个属性将在树的根节点被测试”开始； 使用统计测试来确定每一个实例属性单独分类训练样例的能力，分类能力最好的属性作为树的根结点测试（如何定义或者评判一个属性是分类能力最好的呢？这便是下文将要介绍的信息增益 or 信息增益率，这里要说的是信息增益和信息增益率是不同的,ID3基于信息增益来选择最好的属性，而接下来介绍的C4.5则是基于增益率来进行选择，这也是它进步的地方）。 然后为根结点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支（也就是说，样例的该属性值对应的分支）之下。 重复这个过程，用每个分支结点关联的训练样例来选取在该点被测试的最佳属性。 这形成了对合格决策树的贪婪搜索，也就是算法从不回溯重新考虑以前的选择。 实例讲解分析回归决策树的基本知识，构建一个决策树主要有以下三个重要问题： 数据是怎么分裂的 如何选择分类的属性 什么时候停止分裂 从上述三个问题和ID3算法思想出发， 以一个实际例子对ID3算法进行讲解。 问题描述：我们统计了14天的气象数据(指标包括outlook，temperature，humidity，windy)，并已知这些天气是否打球(play)。如果给出新一天的气象指标据:sunny,cool,high,TRUE，判断一下会不会去打球。 table 1 outlook temperature humidity windy play sunny hot high FALSE no sunny hot high TRUE no overcast hot high FALSE yes rainy mild high FALSE yes rainy cool normal FALSE yes rainy cool normal TRUE no overcast cool normal TRUE yes sunny mild high FALSE no sunny cool normal FALSE yes rainy mild normal FALSE yes sunny mild normal TRUE yes overcast mild high TRUE yes overcast hot normal FALSE yes rainy mild high TRUE no 现在我们用ID3算法来讲解：预备知识讲解：1. 信息熵 2. 信息增益 决策树决策决策树的形式类似于“如果天气怎么样，去玩；否则，怎么着怎么着”的树形分叉。那么问题是用哪个属性（即变量，如天气、温度、湿度和风力）最适合充当这颗树的根节点，在它上面没有其他节点，其他的属性都是它的后续节点。 那么借用上面所述的能够衡量一个属性区分以上数据样本的能力的“信息增益”（Information Gain）理论。 如果一个属性的信息增益量越大，这个属性作为一棵树的根节点就能使这棵树更简洁，比如说一棵树可以这么读成，如果风力弱，就去玩；风力强，再按天气、温度等分情况讨论，此时用风力作为这棵树的根节点就很有价值。如果说，风力弱，再又天气晴朗，就去玩；如果风力强，再又怎么怎么分情况讨论，这棵树相比就不够简洁了。 用熵来计算信息增益，如上图例子： 1. 计算分类系统的熵 类别是 "是否出去玩"。取值为yes的记录有9个，取值为no的有5个，即说这个样本里有9个正例，5个负例，记为S(9+,5-)，S是样本的意思(Sample)。那么P(c1) = 9/14, P(c2) = 5/14 这里熵记为Entropy(S),计算公式为： $$ Entropy(S)= -\frac{9}{14}*log_2{\frac{9}{14}} - \frac{5}{14}*log_2{\frac{5}{14}} $$ 2. 分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益 我们来计算Wind的信息增益: 当Wind固定为Weak时：记录有8条，其中正例6个，负例2个；同样，取值为Strong的记录6个，正例负例个3个。我们可以计算相应的熵为： $$ Entropy(Weak)= -\frac{6}{8}*log_2{\frac{6}{8}} - \frac{2}{8}*log_2{\frac{2}{8}} $$ $$ Entropy(Strong)= -\frac{3}{6}*log_2{\frac{3}{6}} - \frac{3}{6}*log_2{\frac{3}{6}} $$ 现在就可以计算出相应的信息增益了： $$ Gain(Wind)=Entropy(S)-\frac{8}{14}*Entropy(Weak)-\frac{6}{14}*Entropy(Strong)$$ $$ =0.940-\frac{8}{14}*0.811-\frac{6}{14}*1.0=0.048 \qquad \quad $$ 这个公式的奥秘在于，8/14是属性Wind取值为Weak的个数占总记录的比例，同样6/14是其取值为Strong的记录个数与总记录数之比。 同理，如果以Humidity作为根节点： $ Entropy(High)=0.985 $;\quad $ Entropy(Normal)=0.592$ $$ Gain(Humidity)=0.940-\frac{7}{14}*Entropy(High)-\frac{7}{14}*Entropy(Normal)=0.151 $$ 以Outlook作为根节点： $ Entropy(Sunny)=0.971 ; \quad Entropy(Overcast)=0.0 ; \quad Entropy(Rain)=0.971 $ $$ Gain(Outlook)=0.940-\frac{5}{14}*Entropy(Sunny)-\frac{4}{14}*Entropy(Overcast)-\frac{5}{14}*Entropy(Rain)=0.247 $$ 以Temperature作为根节点： $ Entropy(Cool)=0.811 ; \quad Entropy(Hot)=1.0 ; \quad Entropy(Mild)=0.918 $ $$ Gain(Temperature)=0.940-\frac{4}{14}*Entropy(Cool)-\frac{4}{14}*Entropy(Hot)-\frac{6}{14}*Entropy(Mild)=0.029 $$ 这样我们就得到了以上四个属性相应的信息增益值： $ Gain(Wind)=0.048 ；\quad Gain(Humidity)=0.151 ；$ $ Gain(Outlook)=0.247 ；\quad Gain(Temperature)=0.029 $ 最后按照信息增益最大的原则选Outlook为根节点。子节点重复上面的步骤。这颗树可以是这样的，它读起来就跟你认为的那样： ID3优点是理论清晰、方法简单、学习能力较强，但也存在一些缺点： （1）只能处理分类属性的数据，不能处理连续的数据； （2）划分过程会由于子集规模过小而造成统计特征不充分而停止； （3）ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是 倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。 C4.5算法讲解 这一个算法，就不讲解了，有一篇博客讲的特别好，推荐看一下。 C4.5算法讲解博客地址： 总结 因为最近要进行数据挖掘的课程考试，所以在复习（其实是预习...）时，进行总结。我是观看了几篇博客讲解后进行总结的，主要是为了记录我学到的，后面可以进行回头查阅，如能帮助到其他人，那再好不过了。 参考文献 分类算法之决策树ID3详解 决策树分类算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试]]></title>
    <url>%2F%E6%B5%8B%E8%AF%952%2F</url>
    <content type="text"><![CDATA[文本居中引用Any content (support inline tags too). 我是白色的字体，背景是色的~ fairest creatures beauty's rose bear his memory abundance lies sweet self too cruel tender churl mak'st waste in niggarding To eat the world's due, by the grave and thee First unique name 1First unique name 2First unique name 3This is Tab 1. This is Tab 2. This is Tab 3. ButtonUsage: text Alias: text Button with textText Text Text & Title Button with icon Button with text and iconText & Icon (buggy) Text & Icon (fixed width) Text & Large Icon Text & Large Icon & Title Button inside other tagText &amp; IconText Button margin Almost Over Test is finished. 参考文献：https://almostover.ru/2016-01/hexo-theme-next-test/ 组合应用 Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus hendrerit. Pellentesque aliquet nibh nec urna. In nisi neque, aliquet vel, dapibus id, mattis vel, nisi. Sed pretium, ligula sollicitudin laoreet viverra, tortor libero sodales leo, eget blandit nunc tortor eu nibh. Nullam mollis. Ut justo. Suspendisse potenti. Pellentesque fermentum dolor. Aliquam quam lectus, facilisis auctor, ultrices ut, elementum vulputate, nunc. Sed egestas, ante et vulputate volutpat, eros pede semper est, vitae luctus metus libero eu augue. Morbi purus libero, faucibus adipiscing, commodo quis, gravida id, est. Sed lectus. Praesent elementum hendrerit tortor. Sed semper lorem at felis. Vestibulum volutpat, lacus a ultrices sagittis, mi neque euismod dui, eu pulvinar nunc sapien ornare nisl. Phasellus pede arcu, dapibus eu, fermentum et, dapibus sed, urna. Morbi interdum mollis sapien. Sed ac risus. Phasellus lacinia, magna a ullamcorper laoreet, lectus arcu pulvinar risus, vitae facilisis libero dolor a purus. Sed vel lacus. Mauris nibh felis, adipiscing varius, adipiscing in, lacinia vel, tellus. Suspendisse ac urna. Etiam pellentesque mauris ut lectus. Nunc tellus ante, mattis eget, gravida vitae, ultricies ac, leo. Integer leo pede, ornare a, lacinia eu, vulputate vel, nisl. 代码插入java 文件所在位置： \blog 12345678910111213141516/** * @author John Smith &lt;john.smith@example.com&gt;*/package l2f.gameserver.model;public abstract class L2Char extends L2Object &#123; public static final Short ERROR = 0x0001; public void moveTo(int x, int y, int z) &#123; _ai = null; log("Should not be called"); if (1 &gt; 5) &#123; // wtf!? return; &#125; &#125;&#125; 主题自带样式note标签且不问结果如何，尽自己之所能，积极的面对。 default primary Any content (support inline tags too). info warning danger danger no-icon 引用样式 内容 内容 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 警告系列 本文旨在介绍样式的使用规则。 本文旨在介绍样式的使用规则。 本文旨在介绍样式的使用规则。 本文旨在介绍样式的使用规则。 Usage: 百度Alias: text url title 主题自带样式 FontAwesome &lt;/i&gt; 支持 MarkdownHexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。 &lt;/i&gt; 一件部署只需一条指令即可部署到 Github Pages，或其他网站。 &lt;/i&gt; 丰富的插件Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade，CoffeeScript。 采用的是 Font Awesome 的图标，下面给出一些简单的使用例子，更多请查看官网的使用示例。 铅笔 上传 下载 下载 下载变大 33% 下载两倍大 增加下载图标 Download Now 视频连接 YouTube 添加emoji😀😅 🙏 文字背景块样式1. 啦啦啦啦 啦啦啦啦 啦啦啦啦 啦啦啦啦 插入PDF文档以及图片插入PDF文档：将相应的PDF文档放在与博客标题同名的文件夹内，然后再按照如下方式进行插入。 // 在/hexo/blog/public这个文件夹下找到你的博客同名的文件夹，然后将该文件放进去就可以在网页上访问了 点我，这里是PDF文档 表格表头表格的表头使用标签进行定义。大多数浏览器会把表头显示为粗体居中的文本： Header 1 Header 2 row 1, cell 1 row 1, cell 2 row 2, cell 1 row 2, cell 2]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 配置1--markdown文章常用功能]]></title>
    <url>%2FHexo-%E9%85%8D%E7%BD%AE1-markdown%E6%96%87%E7%AB%A0%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[文本居中引用 此标签将生成一个带上下分割线的引用，同时引用内文本将自动居中。 文本居中时，多行文本若长度不等，视觉上会显得不对称，因此建议在引用单行文本的场景下使用。 例如作为文章开篇引用 或者 结束语之前的总结引用。 使用方式 HTML方式：使用这种方式时，给img添加属性class=&quot;blockquote-center&quot;即可。 标签方式：使用centerquote或者简写cq。 效果展示 代码如下： 12345678&lt;!-- 标签别名 --&gt;&#123;% cq %&#125; 人的一切痛苦，本质上都是对自己的无能的愤怒 &lt;font color=black size=4 face=&quot;黑体&quot;&gt;冯相如 &lt;/font&gt;&#123;% endcq %&#125; 这句话与签名者姓名（冯相如）中间间隔的行数越多（回车越多），则最后效果中间间隔就会越大。 至于签名者姓名，你可以用markdown中的字体大小颜色进行设置，如： 123456&lt;font face=&quot;黑体&quot;&gt;我是黑体字&lt;/font&gt;&lt;font face=&quot;微软雅黑&quot;&gt;我是微软雅黑&lt;/font&gt;&lt;font face=&quot;STCAIYUN&quot;&gt;我是华文彩云&lt;/font&gt;&lt;font color=#0099ff size=12 face=&quot;黑体&quot;&gt;黑体&lt;/font&gt;&lt;font color=#00ffff size=3&gt;null&lt;/font&gt;&lt;font color=gray size=5&gt;gray&lt;/font&gt; 但是不要误解，这句话不是我说的，我只不过是为了测试“标签签名”功能。 Bootstrap Callout使用方式1&#123;% note class_name %&#125; Content (md partial supported) &#123;% endnote %&#125; 其中，class_name可以是以下列表中的一个值： default primary success info warning danger 效果示例 对应代码如下； 12345&#123;% note class_name %&#125; Content of class_name &#123;% endnote %&#125;&#123;% note default %&#125; Content of default &#123;% endnote %&#125;&#123;% note primary %&#125; Content of primary &#123;% endnote %&#125; 突破容器宽度限制的图片当使用此标签引用图片时，图片将自动扩大 26%，并突破文章容器的宽度。 此标签使用于需要突出显示的图片, 图片的扩大与容器的偏差从视觉上提升图片的吸引力。 此标签有两种调用方式（详细参看底下示例）： 使用方式 HTML方式：使用这种方式时，为img添加属性 class=&quot;full-image&quot;即可。 标签方式：使用fullimage 或者简写fi，并传递图片地址、 alt 和 title 属性即可。 属性之间以逗号分隔。 效果示例 添加emoji表情使用方式我的是Mac电脑，所以直接用快捷键，control+command+space调出表情框，然后点击输入即可；对于其他电脑，应该也可以直接搜到复制到markdown就好了。 效果示例🤣😜🤨🤩😎🐸🚗❤️🇨🇳 总结我会持续更新，今天先写到这儿，有什么不对，请指正。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试博客照片墙功能]]></title>
    <url>%2FphotoPostName%2F</url>
    <content type="text"></content>
      <categories>
        <category>相册</category>
      </categories>
      <tags>
        <tag>照片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow中的维度和shape的理解]]></title>
    <url>%2FTensorFlow%E4%B8%AD%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8Cshape%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[TensorFlow 的名字已经说明了它最重要的两个概念—Tensor和Flow。Tensor就是张量，Flow翻译成中文就是“流”，它直观的表达了张量之间通过计算相互转化的过程。 Tensor在TensorFlow中是N维矩阵，所以涉及到Tensor的方法，也都是矩阵的处理。由于十多位，在TensorFlow中Tensor的流动过程就涉及到了升维降维。 该篇通过讲解Tensor张量中的shape的理解和一些接口的使用，来体会Tensor的维度概念。 理解TensorFlow的shape TensorFlow和python中的numpy库一样，读shape时应该从外向内读。 1[[1,2,3],[4,5,6]] 和 12[[1,2,3], [4,5,6]] 是一样的，都是2行三列(shape(2,3))。 这该怎么理解呢？首先拿掉最外层的中括号，变成了[1,2,3],[4,5,6]这两个元素，每个元素（如[1,2,3]）拿掉中括号后，剩下1，2，3这三个元素，所以shape=[2,3]。 也就是说shape是从外向内读，然后记录下每一层（即每一个中括号）的元素个数。 比如，shape=[1,1,2]，则表示的数据应该是这样的：[[[a,b]]]. 什么是维度？什么是轴（axis）？什么是维度？维基百科说： 维度，又称维数，是数学中独立参数的数目。在物理学和哲学的领域内，指独立的时空坐标的数目。 0维是一个点，没有长度。1维是线，只有长度。2维是一个平面，是由长度和宽度（或曲线）形成面积。3维是2维加上高度形成的“体积面”。虽然一般人习惯了整数维，但在分形中维度不一定是整数，可能会是一个非整的有理数或者无理数。 看上面的话，可能很费解，我也是！那么就接着往下看吧，会越来越简单的。 总结：维度是用来索引一个多维数组中某个具体数所需要最少的坐标数量。 这句话很有深度，但是你多读几遍，加上下面的例子，你肯定会有自己的理解。 不同的维度的形式如下： 0维，又称0维张量，数字，标量：1 1维，又称1维张量，数组，vector：[1,2,3] 2维，，又称2维张量，矩阵，二维数组：[[1,2],[3,4]] 3维，又称3维张量，立方，三维数组：[[[1,2],[3,4]],[[5,6],[7,8]]] n维，你应该可以自己总结出来了吧！ 再多的维只不过是把上一个维度当做自己的元素 所以看一个张量是几维的，就看最深的数字外有几个中括号！ 什么是axis axis 是多维数组每个维度的坐标 也就是说，对于[ [[1,2], [3,4]], [[5,6], [7,8]] ]这个3维情况，[[1,2],[[5,6]], [[3,4], [7,8]]这两个矩阵的axis是0，[1,2]，[3,4]，[5,6]，[7,8]这4个数组的axis是1，而1，2，3，4，5，6，7，8这8个数的axis是2。 Tensor维度理解 这里通过tf.reduce_mean来理解一下维度 tf.reduce_meanreduce_mean( input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None ) 计算Tensor各个维度元素的均值，这个方法是通过输入参数axis的维度上减少输入input_tensor的维度。 举个例子： x = tf.constant([1.,1.],[2.,2.]) tf.reduce_mean(x) # 1.5 tf.reduce_mean(x,0) # [1.5, 1.5] tf.reduce_mean(x,1) # [1., 2.] x 是 二维数组： 当axis参数取默认值时，计算整个数组的均值：(1.+1.+2.+2.)/4 = 1.5 当axis取0，意味着对列取均值：[1.5, 1.5] 当axis去1，意味着对行取均值：[1.0, 2.0] 在换一个 3*3 的矩阵： sess = tf.Session() x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) print(sess.run(x)) print(sess.run(tf.reduce_mean(x))) print(sess.run(tf.reduce_mean(x, 0))) print(sess.run(tf.reduce_mean(x, 1))) 输出结果为： [[ 1. 2. 3.] [ 4. 5. 6.] [ 7. 8. 9.]] 5.0 [ 4. 5. 6.] [ 2. 5. 8.] 如果我再加一维会是怎么样计算？ sess = tf.Session() x = tf.constant([[[1., 1.], [2., 2.]], [[3., 3.], [4., 4.]]]) print(sess.run(x)) print(sess.run(tf.reduce_mean(x))) print(sess.run(tf.reduce_mean(x, 0))) print(sess.run(tf.reduce_mean(x, 1))) print(sess.run(tf.reduce_mean(x, 2))) 我给输入的Tensor是三维数组： [[[ 1. 1.] [ 2. 2.]] [[ 3. 3.] [ 4. 4.]]] 推测一下，前面的二维经过处理都变成了一维的，也就是经过了一次降维，那么现在的三维或许应该变成了二维。但是现在多了一维，应该从哪个方向做计算呢？首先看输出结果： 2.5 # x [[ 2. 2.] # x,0 [ 3. 3.]] [[ 1.5 1.5] # x,1 [ 3.5 3.5]] [[ 1. 2.] # x,2 [ 3. 4.]] 发现： 当axis参数取默认值时，依然是计算整个数组的均值：(float)(1+2+3+4+1+2+3+4)/8=2.5 当axis取0，计算方式是： [[(1+3)/2, (1+3)/2], [(2+4)/2, (2+4)/2]] 当axis取1，计算方式是： [[(1+2)/2, (1+2)/2], [(3+4)/2, (3+4)/2]] 当axis取2，计算方式是： [[(1+1)/2, (2+2)/2], [(3+3)/2, (4+4)/2]] 总结： 规律： 对于k维的， tf.reduce_mean(x,axis=k-1)的结果是对最里面的一维所有元素求和取平均 tf.reduce_mean(x,axis=k-2)的结果是对倒数第二层里的向量对应的元素进行求和取平均 tf.reduce_mean(x,axis=k-3)的结果是对倒数第三层的每个向量对应元素进行求和取平均 参考链接 附图理解： 在TensorFlow 1.0版本中，reduction_indices被改为了axis，在所有的reduce_xxx系列操作中，都有axis也就是reduction_indices这个参数。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分式方程应用各个类型案例讲解]]></title>
    <url>%2F%E5%88%86%E5%BC%8F%E6%96%B9%E7%A8%8B%E5%BA%94%E7%94%A8%E9%A2%98%E5%88%86%E7%B1%BB%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、【行程中的应用性问题】例1 甲、乙两个车站相距96千米，快车和慢车同时从甲站开出，1小时后快车在慢车前12千米，快车比慢车早40分钟到达乙站，快车和慢车的速度各是多少？ === 分析：等量关系是什么？ —- 慢车用时 = 快车用时 + $\frac{40}{60}$ 所行距离 速度 时间 快车 96 km x km/h 慢车 96 km x-12 km/h 解：设快车速度为 x km/h，则慢车的速度为 （x - 12）km/h 所以： \frac{96}{x-12} = \frac{96}{x} + \frac{40}{60}解得：x = 48 km/h , x - 12 = 36 km/h 答：快车速度为 48 km/h , 慢车速度为 36 km/h。 例2 甲、乙两地相距828km，一列普通快车与一列直达快车都由甲地开往乙地，直达快车的平均速度是普通快车平均速度的1.5倍．直达快车比普通快车晚出发2h，比普通快车早4h到达乙地，求两车的平均速度． == 解：设普通快车车的平均速度为 x km／h，则直达快车的平均速度为 1.5x km／h，依题意，得： \frac{828-6x}{x} = \frac{828}{1.5x}解得：x = 46. 经检验，x = 46 是方程的根，且符合题意。 ∴ x = 46，1.5x = 69 答：普通快车车的平均速度为46km／h，直达快车的平均速度为69km／h。 例3 A、B两地相距87千米，甲骑自行车从A地出发向B地驶去，经过30分钟后，乙骑自行车由B地出发，用每小时比甲快4千米的速度向A地驶来，两人在距离B地45千米C处相遇，求甲乙的速度。 分析： 所行距离 速度 时间 甲 87-45 km x km/h 乙 45 km x+4 km/h 等量关系：：甲用时间=乙用时间+ $\frac{30}{60}$ （小时） 解：设甲的速度为 x km/h ，则乙的速度为 (x+4) km/h 所以， \frac{87-45}{x} = \frac{45}{x+4} + \frac{30}{60}解得：$x{1} = -24$（不符合题意，舍去）$x{2} = 14 km/h$，所以乙的速度为 18 km/h 答：……. 例4 某客车从甲地到乙地走全长480Km的高速公路，从乙地到甲地走全长600Km的普通公路。又知在高速公路上行驶的平均速度比在普通公路上快45Km，由高速公路从甲地到乙地所需的时间是由普通公路从乙地到甲地所需时间的一半，求该客车由高速公路从甲地到乙地所需要的时间。 二、【工程类应用性问题】例1 甲乙两个工程队合作一项工程，两队合作2天后，由乙队单独做1天就完成了全部工程。已知乙队单独做所需天数是甲队单独做所需天数的 $\frac{3}{2}$ 倍，问甲乙单独做各需多少天？ 解：设甲单独做需要 x 天，则乙单独做需要 $\frac{3x}{2}$天，则： \frac{2}{x} + \frac{3}{\frac{3x}{2}} = 1解得：x = 4，则$\frac{3x}{2} = 6$ 答：甲单独做需要4天，乙单独做需要6天。 分析：这个等式的意思是什么？我们知道设 甲单独做需要 x 天，则按照题意，甲在这个项目中一共做了2天，则这两天完成了这个项目的百分之多少呢？ 不就是 $\frac{2}{x}$；则乙在这个项目中一共做了 3天，则这三天做了整个项目的百分之多少呢？ 不就是 $\frac{3}{\frac{3x}{2}}$。 所以加起来就是把这个项目完成了百分之百，即1. 所以等式为： \frac{2}{x} + \frac{3}{\frac{3x}{2}} = 1例2 甲、乙两个施工队共同完成某居民小区绿化改造工程，乙队先单独做2天后，再由两队合作10天就能完成全部工程．已知乙队单独完成此项工程所需天数是甲队单独完成此项工程所需天数的$\frac{4}{5}$，求甲、乙两个施工队单独完成此项工程各需多少天？ 解：设甲施工队单独完成此项工程需x天，则乙施工队单独完成此项工程需$\frac{4x}{5}$天，根据题意，得: \frac{10}{x}＋\frac{12}{\frac{4x}{5}}＝1解这个方程，得x＝25，经检验，x＝25是所列方程的根，所以 $\frac{4x}{5} = 20$ 答：甲单独完成需要25天，乙单独完成需要20天。 三、【营销类应用性问题】 营销类应用性问题，涉及 进货价、售货价、利润率、单价、混合价、赢利、亏损 等概念，要结合实际问题对它们表述的意义有所了解，同时，要掌握好基本公式，巧妙建立关系式．随着市场经济体制的建立，这类问题具有较强的时代气息，因而成为中考常考不衰的热点问题． 例1 某校办工厂将总价值为2000元的甲种原料与总价值为4800元的乙种原料混合后，其平均价比原甲种原料每千克少3元，比乙种原料每千克多1元，问混合后的单价每千克是多少元？ 解：设混合后的单价为每千克 x 元，则甲种原料的单价为每千克 (x+3) 元，混合后的总价值为 $(2000＋4800)$ 元，混合后的重量为 $\frac{2000+4800}{x}$ 斤，甲种原料的重量为 $\frac{2000}{x+3}$ ，乙种原料的重量为 $\frac{4800}{x-1}$，依题意，得： \frac{2000}{x+3} + \frac{4800}{x-1} = \frac{2000+4800}{x}解得：x = 17，经检验，x = 17 是原方程的根，所以 x=17．即混合后单价每千克为17元。 答：…….. 例2 某商场销售某种商品，一月份销售了若干件，共获得利润30000元;二月份把这种商品的单价降低了 0.4元，但是销售量比一月份增加了5000件，从而获得利润比一月份多2000元，调价前每件商品的利润为多少元？ == 分析： 可以列出三个等量关系 1．2月份销售量一1月份销售量=5000 2．2月份销售量×2月份利润=2月份总利润 3．1月份利润一2月份利润=0.4 解：由题意知，根据 二月所获利润 = 30000 + 2000 等式列方程，设调价前每件商品利润为 x 元，则调价后即2月份每件商品利润为 （x-0.4）元，得： (x-0.4)(\frac{30000}{x} + 5000) = 30000 + 2000化简得：$5x^2 -4x -12 = 0$，所以$(x-2)(5x+6) = 0$ 解得 x=2 或 x=-1.2（舍去） 答：调价前每件商品利润是2元。 例三 某商厦进货员预测一种应季衬衫能畅销市场，就用8万元购进这种衬衫，面市后果然供不应求，商厦又用17.6万元购进了第二批这种衬衫，所购数量是第一批购进量的2倍，但单价贵了4元，商厦销售这种衬衫时每件定价都是58元，最后剩下的150件按八折销售，很快售完，在这两笔生意中，商厦共赢利多少元。 解：由题意知，设商场第一次购进 x 件衬衫，则第二次购进 2x 件， \frac{80000}{x} = \frac{176000}{2x} -4解得：x = 2000，经检验，x=2000是这个方程的根，∴ 2x=4000。 商场利润 = 两次总共所买获得总钱 - 两次进购所花总钱。 (2000+4000-150)*58+58*0.8*150 - 80000 - 176000 = 90260 （元）答：在这两笔生意中，商场总共盈利90260元。 例4 一个批发兼零售的文具店规定：凡一次购买铅笔300枝以上，（不包括300枝），可以按批发价付款，购买300枝以下，（包括300枝）只能按零售价付款。小明来该店购买铅笔，如果给八年级学生每人购买1枝，那么只能按零售价付款，需用120元，如果多购买60枝，那么可以按批发价付款，同样需要120元， （1）这个八年级的学生总数在什么范围内？ （2）若按批发价购买6枝与按零售价购买5枝的款相同，那么这个学校八年级学生有多少人？ 解：（1）由题意知，设八年级总人数为 x 人，每人购买一支，只能按零售价，所以 x≤300，如果多购买60支，则可按批发价，则 x + 60 &gt; 300 所以， 240 &lt; x ≤ 300 （2）根据按批发价购买6支与零售价购买5支的所需钱相同，列等式求解： \frac{120}{x}*5 = \frac{120}{x+60}*6解得，x = 300，经检验，x=300是原方程的根。 答：这个学校八年级人数有300人。 例5 某种商品价格，每千克上涨1/3，上回用了15元，而这次则是30元，已知这次比上回多买5千克，求这次的价格。 例6 小明和同学一起去书店买书，他们先用15元买了一种科普书，又用15元买了一种文学书，科普书的价格比文学书的价格高出一半，因此他们买的文学书比科普书多一本，这种科普和文学书的价格各是多少 四、【轮船顺逆水应用问题】例1 轮船顺流、逆流各走48千米，共需5小时，如果水流速度是4千米/小时，求轮船在静水中的速度。 === 分析：顺流速度=轮船在静水中的速度+水流的速度 逆流速度=轮船在静水中的速度-水流的速度 解：设轮船在净水中的速度为 x km/h，得： \frac{48}{x+4}+\frac{48}{x-4} = 5解得：x = 20 或 x=-0.8（舍去） 答：船在静水中速度为 20 km/h。 例2 例2 轮船在顺水中航行30千米的时间与在逆水中航行20千米所用的时间相等，已知水流速度为2千米／时，求船在静水中的速度。（10） 例3 轮船顺水航行80千米所需要的时间和逆水航行60千米所用的时间相同。已知水流的速度是3千米/时，求轮船在静水中的速度。 例4 已知一个汽船在顺流中航行46千米和逆流中航行34千米共用去的时间正好等于它在静水中航行80千米用去的时间并且水流的速度是每小时2千米求汽船在静水中的速度。 例5 9、一艘轮船在静水中的最大航速为30千米/时，它沿江以最大航速顺流航行100千米所用的时间与以最大航速逆流航行60千米所用的时间相等，问：江水的流速为多少？设江水的流速为x千米/时，则可列方程? 五、【其他应用性问题】例1 例1 要在15%的盐水40千克中加入多少盐才能使盐水的浓度变为20%． 分析：设加入盐千克．浓度问题的基本关系是：$\frac{溶质}{溶液}=浓度$ 解：设应加入盐 x 千克，依题意，得 \frac{40*15\%+x}{40+x} = 20\%解得：x=2.5，经检验，x=2.5是原方程的根 答：需要加入2.5kg的盐。 例2 某校招生录取时，为了防止数据输入出错，2640名学生的成绩数据分别由两位程序操作员各向计算机输入一遍，然后让计算机比较两人的输入是否一致，已知甲的输入速度是乙的2倍，结果甲比乙少用2小时输完。问这两个操作员每分钟各能输入多少名学生的成绩？ 例3 供电局的电力维修工要到30千米远的郊区进行电力抢修，技术工人骑摩托车先走，15分钟后，抢修车装载这所需材料出发，结果他们同时到达，，已知抢修的速度是摩托车的1.5倍。求这两种车的速度。]]></content>
      <categories>
        <category>家教</category>
      </categories>
      <tags>
        <tag>曹晨</tag>
        <tag>家教</tag>
        <tag>案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于网格的运动统计，用于快速、超鲁棒性的匹配]]></title>
    <url>%2FGMS-Grid-base%20Motion%20Statistics%20for%20Fast%EF%BC%8CUltra-robust%20Feature%20Correspondence%2F</url>
    <content type="text"><![CDATA[前序今天分为两个部分讲解： 关于Feature Matching 的介绍，可能有些人对feature matching本身不太了解。 最近几年关于关于feature matching的文章，主要有三篇。 前两篇文章主要的工作是：把feature matching做的特别的robust，之前的一些作法可能能基本的实现基本的match，但是因为match的质量不高，导致很多应用是不能用的，比如三维重构。但是经过前两篇文章，就可以实现之前不能实现的了，提升了鲁棒性，具有了很大的提高。该篇文章主要是对前两篇文章的优化，因为前两篇虽然最终效果很好，但是速度很慢。对于三维重构来说速度勉强可以，但是对于机器人导航、SLAM、无人机这些应用来说，速度就不够了，作者就对这个问题进行了时间的优化，最终达到的效果就是：性能和前两篇文章实现的相近，时间提升了很多，很快。希望能把这个算法应用到更多实时应用上。 Feature Matching Introduction 我们可以看一下这两张图，图中山峰是同一个山峰，但是是由人由两个角度进行拍照。所以我们如果能把两张图片中对应的点找来，找出来有什么用呢？我们知道上面的两幅图是由人从不同的角度拍照得来的，那么这两幅图中间是有一个几何关系，这个几何关系可以由一个方程来表示。那么每一个匹配点就相当于这个方程的解，如果我们有足够多的正确匹配的点，我们就可以用这些点来估算这个方程里面的参数，相当于我们知道了两张图片中对应的几何关系 Correct Correspondences的应用你把两张图片中相同的点匹配起来有什么用？ Geometry between 2 views，Geometry就是两张图片中的几何关系，这样我们便可以用来做很多事情：比如：Estimate Camera Pose Localization(SFM)、Tracking(SLAM):就是你能把一帧一帧图像之间的相对应的几何关系能算出来，如果很准的话，你就能画出来人运动的轨迹，就是一个人导航问题。 Similarity（Number of matches），根据Feature matching的多少评测两张图片的相似度。然后我们就可以用来做Image Retrieval(图像检索)、Object Recognition(目标识别)、Loop Closing(SLAM)…. 如何实现Feature Matching就是在两张Image中相同的这些部分找到，再把它们匹配起来。 首先是如何找到，包含两个步骤：Detection、Description。 Detection：首先找到的是角点，再者比较容易找的是边缘的点。 Description：对于这些每一个点计算出周围的特征，比如sift是用128维的数字去描述他的一个description。 matching的时候，只要用这个128维的数字计算跟哪个点的距离最小，就是最相似的 Geometry：你拿到这些点的匹配后，去做一个他们之间的相关的几何匹配，模拟几何关系，符合这个几何关系的就是正确匹配的点，不符合的就是错误的点。有了几何关系就可以坐后面的事情了。 LIFT：是用深度学习做的，他认为用深度学习得到的feature比手动提取的feature 更加有用 Matching Algorithms：CODE、RepMatch、GMS。因为Brute-Force和Approximate（FLANN）所获取的匹配点是杂乱的，大量的，这样你最后算几何关系就会很慢，效果还不好。我们这些Matching Algorithms算法是用来从Nearest-Nerighbor match中找出正确的匹配点，错的剔除去。 如何估计两张图片之间的几何关系一般用的是RANSAC-based方法。 RANSAC：Random Sample Consensus，它是根据一组包含异常数据的样本数据集，计算出数据的数据模型参数，得到有效样本数据的算法。 RANSAC也做了一下假设：给定一组（通常很小的）局内点（符合最优模型的点为局内点，不符合的定义为据外点），存在一个可以估计模型参数的过程而该模型能够解释或者适用于局内点 一个简单的例子就是从一组观测数据中找到合适的2维直线。假设检测到的数据如上图左图。简单的最小二乘法不能找出适应与局内点的直线，原因是最小二乘法尽量去适应包括局外点在内的所有的点。相反，RANSAC能够得出一个仅仅用局内点计算出模型，并且概率还足够高。但是RANSAC并不能保证结果一定正确，为了保证算法有足够搞得合理概率，我们必须小心的选择算法的参数。 在我们的Feature matching里面，它的RANSAC就不是一条直线了，而是两张图片中的几何关系。这里有两个普遍的几何关系： Fundamental Matrix（for 3D scenes）：Point to Line(weak, general)只能是点对线的匹配，就是给定第一张图片中的一个点，它会在第二张图片中画出这个点所在的线。能把一个点和一条线对应起来，这就是它们之间的几何关系，这个几何关系能够恢复出来相机的旋转和平移，用的方法主要是RANSAC方法 Homography (for 2D scenes)：Point to Point（strong，narrow range）只对2D场景有效，比如你拍一张墙的照片，然后对其中一个点，就能匹配到第二张图片的点。 Recent Robust MatchersCODE [1] 该算法解决的是 wide-baseline matching问题。 wide-baseline matching ？基线的本意是指立体视觉系统中量摄像机光心之间的距离。一句拍摄两幅图，像的视点位置可将对应点匹配问题分为宽基线和窄基线。宽基线一词用于匹配时，泛指两幅图像有明显不同的情况下匹配。产生这种情况的原因有可能为摄像机之间的位置相差很大，也有可能由于摄像机旋转或焦距的变化等因素产生。 每一幅图片中的其实是两幅图片，图片中的墙是相同的一个墙，只是由于拍摄的角度特别的大，传统的一些算法处理的不是很好，但是code算法可以，匹配的比较多，准确率也比较高 Idea 整体思想如上图，包含了三个回归模型，likelihood是第一个模型，affine motion 为第二第三个模型。这些回归模型就相当于分类器。 首先输入的是 Selected matches ，就是先挑一些比较好的match，把它们记录下来，去fit第一个模型，其实就相当于是一个分类器，然后用这个分类器把所有match做一个过滤，符合这个的留下来，不符合的剔除。 然后再用后面两个模型（即分类器）去过滤，最后都通过的才算最终正确的匹配点。 这三个回归是怎么做的？看下图（回归模型） 有一堆散的点，用这些数据去fit一个平面，这个平面上的你认为是对的，不在这个平面上，你认为是错误的。这样的regression总共有三步。 Likelihood train data：是我们选出来的那些好的match test data：是所有的的match features of a correspondence：把每一个match点定义为8位的向量，x,y是它的空间位置，dx，dy描述了它的motion，就是一对match点，左右各有一个点，用第二个点（第二幅图）的x2 减去 第一个点x1得到dx。T1-T4是一个矩阵的变换。我们看到的是点对点的匹配，其实是区域的匹配，这四个参数就是描述了左边的这个小区域到右边的对应的小区域的变换。 Label：我们开始初始化为都是1，认为都是对的。 Non-linear Optimization：描述了所有correspondence之间的相似性，相似性大的就是1，小的就是0 likelihood模型为什么有用，它就是用来区分连续的Motion和不连续的Motion，作者认为不连续的Motion一般就是错的。第一个模型就是找那些correspondened是连续的。第二个模型就是把它的精度提升了一下，把x和y都预测了一下。作者认为 (a) (b) 这样的图像中的点是正确的点，而像 (c) 这两个比较近的点的方向还差这么大，就认为是错的。第一个模型likelihood就是找到连续的motion，那些点的motion是连续的。第二个模型就是提升了它的精度，把x和y都测了一下。 结果code算法的input就是那个Nearest Neighbor match，它里面是乱七八糟，所有的match都有，而code就是把其中的对的找出来。 应用 主要是三维重构，你上面第二幅图是用商业软件做的，并不是很好，第三幅是用SFM做的，第四幅还是用SFM做的，但是它把SFM中的Feature matching替换成CODE算法得出的feature matching，效果就好很多。 Run time comparison 纵轴是时间表示，以 s 为单位，横轴是matches的数量，可以看到code运行的时间要比其他的好很多，但是对于slam ，机器人导航，一秒钟要读取几十帧，code这个算法的时间还是不够的。 RepMatch [2] 基于第一个算法之上，解决一个重复结构（repeated structure）的问题。它是用第一个算法的输出，然后在这个输出上再做一次优化，筛选一些对的点。 repeated structure到底是一个什么问题 比如 (a) 图中，我们拍了一栋楼的正面，侧面，反面等照片，然后进行SFM三维重构，发现结果它是给我化成了四个部分，但其实它是一栋楼，就是说三维重构并没有重构出这栋楼。原因：因为（a）图像中的楼的各个面的窗口都很像，你也不能确定正面的窗口和背面的窗口有什么不一样，所以导致一个问题：比如SFM就给你标出四个独立的模型，这明明是一栋楼，它合不起来。主要是因为他们之间错误的匹配太多了，你把楼的前面跟后面匹配在一起，它脑子就乱了，根本不知道这是个什么模型。 然后用了RepMatch，就能构造一个完整的模型，周围的每一个点是每一张相片拍照的位置，中间这一圈就是楼的俯视图，你他最后dense reconstruction的结果就是最右边的图 Repetitive Structure（怎么做到的构建成功的）idea:是做分类它是建立在CODE 输出结果之上的。它把输出结果分为了几类： 第一类：把结果放的严一些，然后结果数就很少。 GMS这篇文章主要是把前面两篇文章的主要思想掌握到，然后把它变快，能够把它应用到更多上面。 首先我们看一个小视频，上面的是作者的GMS算法，下面的是sift，我们可以发现，GMS的match首先在数量上就比sift的多很多，这样我们就能保证最后验证那个几何关系比较准，如果点比较少的话，你做RANSAC的话，就会不稳定，出错之类的。作者的算法比sift这个算法更快更好。 Motivation：为什么又要质量好又要速度快 最常用的算法可能就是Ratio Test，比较流行，比较快，但是不具备鲁棒性。当前的算法比如code等，效果很好，但是很慢。GMS特点就是：又快又好。 论文GMS的方法实际上是消除错误匹配的一种方案，比如可以替换ransac。算法执行的大致流程是：先执行任意一种特征点的检测和特征点的描述子计算，论文中采用的是ORB特征。然后执行暴力匹配BF，最后执行GMS以消除错误匹配。 Key IdeaTrue matches(green) are visually smooth while false matches(cyan) are not. 正确匹配的点我们用绿色的标注出来，错误匹配的点用青涩标注出来。首先我们可以看到正确匹配的点，举个例子，假设随便拿出来两个正确匹配的点（绿色标注的）记为1号点和2号点，1、2号点在另一幅图中对应匹配的点为1‘和2’。我们可以看到1号点对应的匹配点1‘与2号点对应的匹配点2’位置差不多，相似，整个过程是一个比较smooth的过程。而两个相近错误的点（青色的标注），对应的匹配点一个去了楼上，一个去了天上，这就是我们从观察上得到的信息。那么如何把这个观察的信息变成一个算法去验证这个点是对还是不对，这就是算法的核心。 首先我们说了一个正确的match看起来比较smooth，所以我们有没有办法把这些看起来比较smooth的match提取出来，我们就认为这些match是对的。根据贝叶斯上讲，既然正确的match看起来比较smooth，那么比较smooth的match应该也是正确的。 Key Idea：核心思想就是我们如何把match中比较smooth的match拿出来。 方法主要分为三部分： Motion Statistics：motion统计 Grid Framework：grid结构是用来加速的 Motion Kernels：也是用来加速的 后面两个部分都是为了实现第一个部分Motion统计，让它变得很快。 Motion StatisticsMotion Statistics Model首先看一下统计模型是怎么样的，对每一个match，比如说match xi，给他左边和右边各画一个圈，数一下圈中还有多少个其他的match 这个有什么道理：如果一个正确的match，它旁边还有正确的match的话，应该是支持它的。（就像之前的图中显示那样，对的，周围一般Motion是相似的）道理就是一个正确的match旁边可能有对的点在帮你，而对于一个错误的match，你的这个错误是随机发生的，你很难再找到一个点跟你犯一样的错误。所以这两个的概率相差非常大的，第一种正确的match这个圈里面总会有一些点来支持它，而错误的match 支持它的点可能有，但是会很少，这就是这个算法的模型。 这个图中Si就是表示统计值，这张图中Si = 2，Sj是下面那个，没有人支持他，所以是0.所以motion统计就是这样一件事情。 Si = 2；Sj = 0. 我们分析一下这个模型为什么对于判断“这个点是否正确”有效我们假设一个点的统计值 Si ， 它应该服从这个binomial distribution（二项分布）。就假设这个 xi 是正确的的话，那么用 pt 表示这个概率，这个概率是指：假设上图中左边的圈为A，右边的圈为B，那么如果这个点 xi 匹配是正确的话，它周围的点从A到B的可能性是多少，就是支持这个点 xi 的可能性是多少。对于这个 pf ，如果这个 xi 匹配是错的，那么它周围的点支持它的概率是多少。 我们用 fa 表示 A 这个圈中额外支持它的n个点中的一个点。然后去算这个概率，我们要给出一个事件和假设，如下图： 第一个事件表示：fa这个点匹配正确，它的概率为 t，这个概率跟这个算法有关，匹配的质量越高，那么这个概率当然要高 第二个事件表示：fa 这个点匹配错误，它的概率就是 1-t 第三个事件表示：fa 这个点 匹配到了 B 圈中的一个点（但不一定是正确的） 定义这三个事件后，我们给出一个假设：如果一个点它匹配错了，它的匹配还是从A到B的话，那么它的概率是多少 。我们知道如果是A中的一个点匹配错了，那有可能匹配到右边Image中的任何一个点，而它恰好匹配到B圈中的点，如果是随机分布的话，那么这个概率应该为 (m / M), m为B圈中的点的个数，M 为右边图中所有的点的个数。但是可能很多Image是这样的分布，但是有一些Image不是，我们为了弥补这个假设（随机分布），我们给它乘以一个 “贝塔”参数，当这个参数比较大的时候，这个假设就不那么成立了 。这个参数为1的时候，这个假设正好成立。 我们算一下这个概率，如下图： 因为m很小，所以 pf 是一个接近 0 的概率。而 pt 是一个接近 t 的数，因为 pf 是接近 0 的数 。所以这两个的差异很大，另外我们要怎么才能让这个差异变得更大？ 我们上面是在两张图中各画了一个小圈AB，这个小的区域中，他们的正确的点motion是连续的，通常一个Image中，不是只有这个一个小的区域中的motion是连续的，更大的区域应该也是连续的。 比如，之前那张狗的图片，我们只是画了狗的鼻子的一小块区域，但是其实鼻子旁边的区域也是连续的，当然我们不是画整张图片，只是比那个小圈更大一些，所以扩展到了 3*3的这样一个区域，相当于比之前的一个圈扩大了9倍，如下图： 我们之前只是画了a、b这两个圈，现在我们画了9个这样圈，并且统计这个 3 * 3 这样的区域中的 Si。这样便能增大这个distribution之间的差异，如下图： 因为是3*3，所以 K=9 ，这个Si的分布中 pt 和 pf 是跟之前的一样的，因为只有K变化了。所以计算一下均值和方差，如下图： 这个是后面要用到。 现在我们分析一下这个模型有什么用 Analysis Model Partionalbility 这个词是表示，对的点和错的点之间有多大的差距，看起来在这个模型中有多么不一样。P就是用来定义这两个分布（错的和正确的分布）的距离。P越大，表示这个错误的点和正确的点看起来越不一样，那么这个模型就越强。 Quantity-Quality equivalence 我们可以看到这个 P 是正比于这个 Kn 的，就是说你用越多的点在里面，那么这个模型就越强，错的点和对的点的区别就越大。 Relationship to Descriptors 看与这个descriptors的关系，这个descriptors就是 t ，比如你用sift特征点，假如这个sift特征特别强的话，那么这个P是趋近于无穷大的。这就能验证我们的算法是对的。，也比较合乎道理，你特征越强，你越好分割，所以 P 趋近于无穷大。 Experiments on real data该模型在Oxford Affine Dataset上进行评估。 在这里，我们运行SIFT匹配并根据基础事实将所有匹配标记为内点或外点。 我们计算每个匹配的一个小区域内的支持的个数。 纵轴表示了每个matching旁边画了两个小圈，然后数一数这个小圈中 正确的Inlier周围中到底有多少个点支持它，错的点到底有几个点支持它。 它有8的小的datasets，每个dataset里面有五对张照片，这五对照片匹配起来一对比一对难，就是越来越难，每个数据，是用8个datasets的数据做了平均得到的。 所以从上图可以看出：对于正确的点，就算是最难得时候，也有接近8个点支持它，而对于错误的点，始终没有超过1个点支持它。所以可以看出这个模型对于正确的点和错误的点的认识性还是很强的。 这个算法我们已经知道了，那么如何去加速实现它呢？ Grid Framework我们刚才是给每对匹配点画两个圈，一个点一个圈，这样的问题就是：你要想知道有多少个点这个圈内，那你必须要把所有的点过滤一遍。 所以现在我们的做法是不要给这个匹配点画圈，我们直接把image 打成网格，然后这个匹配点自然就会落在这个Image的网格中，那么它这个网格就代表他的这个小圈，就没有了两个小圈的匹配，而变成了两个网格的匹配，你只要数一下这个网格中有多少个点跟它一样就可以了。 只要你一开始把这些 Feature matching丢进这些网格中，你很容易数清楚。这样算法的时间复杂度就从O(n) -&gt; O(1). 所以会变得很快。 Motion Kernel因为我们之前已经知道用更多的region会产生更好的结果，因为我们进行统计的时候就变成了统计这九个小网格的正确点的数量就好了，就很容易能数清楚。 Empirical parameters（经验参数） 你用多少个网格？ 太细的话，统计不好 太粗的话，精度太低 经验上作者是说用 20 * 20 How to set the threshold？怎么设置阈值？就是认为计算出来的这个值（用 τ 表示）过了多少是对的，没过多少是错的。用的是一个平均值加一个方差，这是经验得到的，并没有什么道理可言。这个 α 参数一般我们设置为 0.6 。 Grid Motion Statistics Algorithm首先是给定这个 Correspondences（匹配点）、scale、rotation，然后产生这个 Kernel 和 Grid（20 * 20）。确定好了，去计算每一个点的统计值 Si ，然后去计算这个 τ，如果这个 Si &gt; τ，那么这个点就是对的。没过就是错的。 Full feature matching pipeline这是一个完整的流程，首先获取 feature 然后做 match，然后交给GMS，因为这个scale、rotation可能有多个值，你可以多试几遍，找到一个最好的值。 Run time这里是提取10000个点为例。这个值可以再Video上实时的，因为ORB和Nearest neighbor可以并行 Evaluation 我们测试了4个dataset，第一个是TUM。做slam的可能知道，下面是6个视频，然后从中提取出很多帧做测试，所以这个数据集上有3000多个照片。 0-30是表示两张图片的角度，就是baseline。至于在Strecha和VGG上，Ratio算法表现并不差，是因为这两个数据集太水了，大家是用来发paper的。像TUM这样真正的应用数据上来看，一般的算法根本就不太行了，质量会很差。 X轴是时间，用的是ms表示，Y轴是performance，红色的是作者的算法表现。可以看出GMS实现了结果上差不多，但是时间上快很多倍的效果。 我们从上图可以看出，第一幅图，是在平面上，GMS和SIFT比来看，SIFT经过RANSAC后，结果还是勉强可以的。就是说在平面上，这个sift加上RANSAC结果还是可以的。但是在3D中，sift匹配的全都是错误的点，但是GMS依然是表现很好。说明我们这个算法对于wide-baseline问题是真的解决了，不像sift只有在实验中（那些dataset上）表现的还可以接受，但是在现实中的dataset就不行了。 再就是解决了重复结构的问题，如下图： 之前我们是假设物体是静止的，但是这些猫狗是会动的，动了之后，你拍的图片用sift去匹配的话，如果有的匹配错，并且没有那些RANSAC方法去帮它修正的，那么你匹配对的就是对的，错的就是错的，没有办法去修改。而作者的算法是可以匹配正确的，根本不需要去修正。]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GMS</tag>
        <tag>CVPR 2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac常用快捷键教程]]></title>
    <url>%2FMac%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[用option快速输入一些特殊符号 option V： 输入 √ option /： 输入 ÷ option =： 输入 ≠ option &gt;： 输入 ≥ option &lt;： 输入 ≤ option K： 输入 ˚ option X： 输入 ≈ option =： 输入 ≠ option 2： 输入 ™ option R： 输入 ® option Y： 输入 ¥ option P： 输入 π option G： 输入 © option + shift + K：  通知中心开关 按住 option 点击桌面右上角通知中心，即可直接关闭或开启通知。不用在通知中心中打开了。 比如用 Mac 全屏看电影又不想被右上角的消息弹出打扰的话，暂时开启勿扰（关闭通知）就是了. WIFI状态按住 option 点击 Wi-Fi 图标即可查看 IP 地址、Wi-Fi 信号强度等等 Wi-Fi 的详细信息。 通用快捷键 command + M：最小化窗口 command + H：隐藏窗口 command + N：新建窗口 command + O：打开 command + Shift + S：另存为 command + P：打印 command + W：关闭 command + Q：退出 文件管理器 command + I：显示简介 command + F：搜索 command + delete：删除 command + Shift + delete：清空回收站 option + 方向键左：将光标移动到前一个单词 command + option + I：查看多个文件一共有多大 option + 方向键上：将光标移动到当前段落的开头 command + shift + 方向键左/右：可以选中一句话 command + 方向键左：将光标移动到句子的开头 command + 方向键上：将光标快速移动到整篇文本开头 浏览器 command + +\=：放大 command + --：缩小 command + T：新建一个选项卡 command + N：新建一个新窗口 command + R：刷新 command + F：搜索 command + W：关闭当前选项卡 command + D：将网页加入个人收藏 command + Z：恢复刚才关闭的网页 command + shift + B：显示或隐藏收藏栏 command + shift + N：打开一个全新的隐私浏览器窗口比较合适 space / space + shift：向下或向上滚动一整页 command + option + Q：退出 Safari，在下次打开的时候，会自动加载上次的所有窗口 option + 方向键上/方向键下：网页向上或向下滚动一整页 系统 command + option + esc：强制退出程序 command + shift + Y：生成便签 command + space：切换输入法 command：按住command即可用鼠标移动拖动顶部图标排列 control + command + space：快速调出emoji表情和各种特殊符号输入 shift + option + F1 / F2：可以以1/4格为单位对屏幕亮度调整，音量也是如此。 总结后续会继续补充。有什么遗漏的，可以留言。]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吵架]]></title>
    <url>%2F2018.4.27%2F</url>
    <content type="text"><![CDATA[今天是怎么了，或者说这几天是怎么了，天天惹你生气，心情很烦躁，我准备开始写日记，抒发一下自己的心情，要不我可能会被憋死的。 前序环顾四周和手机应用，竟然没有一个地方可以让我肆无忌惮的畅所欲言的发泄。在朋友圈、空间、微博你敢发吗，人长大了，总是要顾虑这顾虑那，我不想发在朋友圈后，让你再多想，我只是想找一个地方写写我的心情，要不我可能真的会被郁闷死的。 回忆中学初中时候，我个人的品性很差，现在想想那可能是最灰暗的三年了，没有知心朋友，脾气古怪，小心眼，自己一个人卑微的活着，也不知道打了多少架，现在想想，心中只是一阵痛。我只能说，有时候真的不是我的错，我还记得初中上学时，学校不让住校，我就只能在学校外跟同学合租房子，大约有5、6个人吧。其中有一个是邻班的班长，有天晚上，他就突然找我的茬。有人可能说，人家怎么会莫名其妙的找你的茬，但我可以摸着良心说，我真的没有做错什么。那天晚上，他跟我打了一架，我被揍了，哭着跑了出去，大晚上自己一个人走在马路上，一个人哭，最终我还是去给我妈妈打了个电话，最终的结果就是，我妈妈来了学校，他妈妈也来了学校，和解。后来我才知道，他为什么要跟我打架，因为他想让他一个好朋友住进来，把我踢出去，现在想起来我还是很痛恨他。 事情原委话扯远了，今天晚上也不知道为啥心情这么暴躁，刚开始和你打游戏时的心态可能就不对，最后因为我想用法师貂蝉，而队伍中已经有个法师，你就说别让我用法师，换个吧。我当时可能心情就很不爽了，因为之前打了几局都不好，然后就反驳，最后凶了你一句，然后我们就各删了游戏。呵呵，然后就在微信上聊了起来，你说你哭了，觉得自己很委屈，只不过是玩个游戏，我就凶你了，那以后生活的不得水深火热。我当时心中还很烦躁，我这个人要是心情烦躁起来，就不会讲理了。 聊着聊着，我就问了你一句：“现在后悔还来得及”，其实说完这句话，我很还忐忑，我心中在期待，你肯定的回答，但是没有。但也不意外，毕竟都在气头上，而且还是我的错。 我很害怕失去你，又不想表达出来，想听你心中最真实的想法，我自己心中的想法是“你要是想离开，我不会挽留”，因为你既然都想好了，我不想要个勉强的你。 自我反省话说回来，也该批评自己了，最近是很膨胀啊。毕竟这几年过得都顺风顺水，没什么大的坎坷，可能自己在心态上就有了变化，脾气也见长了，价值观也有了变化，喜欢购物，花钱，穿好看的。没有了高中那种一花钱就想到父母的辛苦。我觉得这样很不好，我要改变自己，虽然之前删游戏是一时之快，但是我想坚持下来，把更多的时间用在学习上。 改进之处 把更多的时间花在学习和陪你上 脾气暴躁需要改改，以后多读书，去图书馆借本自控力的书，每天阅读半个小时 爱说脏话，这个要改，但是没啥很好的方法，就平时多自己注意吧 总结自己写完这些字，心情果然好了点。继续加油，明天会是美好的一天！加油！]]></content>
      <categories>
        <category>心情笔记</category>
      </categories>
      <tags>
        <tag>丹岑</tag>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[iTerm 2 实用快捷键]]></title>
    <url>%2FiTerm%202%20%E5%AE%9E%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[⌘ + Click：可以打开文件，文件夹和链接 ⌘ + n：新建窗口 ⌘ + t：新建标签页 ⌘ + w：关闭当前页 ⌘ + 数字 &amp; ⌘ + 方向键：切换标签页 ⌥⌘ + 数字：切换窗口 ⌘ + enter：切换全屏 ⌘ + d：左右分屏ß ⇧⌘ + d：上下分屏 ⌘ + ;：自动补全历史记录 ⇧⌘ + h：自动补全剪贴板历史 ⌥⌘ + e：查找所有来定位某个标签页 ⌘ + r &amp; ⌃ + l：清屏 ⌘ + /：显示光标位置 ⌥⌘ + b：历史回放 ⌘ + f：查找，然后用 tab 和 ⇧ + tab 可以向右和向左补全，补全之后的内容会被自动复制， 还可以用 ⌥ + enter 将查找结果输入终端 选中即复制，鼠标中键粘贴 很多快捷键都是通用的，和 Emace 等都是一样的 ⌃ + u：清空当前行 ⌃ + a：移动到行首 ⌃ + e：移动到行尾 ⌃ + f：向前移动 ⌃ + b：向后移动 ⌃ + p：上一条命令 ⌃ + n：下一条命令 ⌃ + r：搜索历史命令 ⌃ + y：召回最近用命令删除的文字 ⌃ + h：删除光标之前的字符 ⌃ + d：删除光标所指的字符 ⌃ + w：删除光标之前的单词 ⌃ + k：删除从光标到行尾的内容 ⌃ + t：交换光标和之前的字符]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>iTem2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密钥分发中心（KDC）]]></title>
    <url>%2F%E5%AF%86%E9%92%A5%E5%88%86%E5%8F%91%E4%B8%AD%E5%BF%83%EF%BC%88KDC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简言 密钥分发中心是一种运行在物理安全服务器上的服务，KDC维护着领域中所有安全主体账户信息数据库。 与每一个安全主体的其他信息一起，KDC存储了仅安全主体和KDC知道的加密密钥，这个密钥也称长效密钥（主密钥），用于在安全主体和KDC之间进行交换。 KDC是作为发起方和接收方共同信任的第三方，因为他维护者一个存储着该域中所有账户的账户数据库，也就是说，他知道属于每个账户的名称和派生于该账户密码的Master Key（主密钥）。而用于Alice和Bob相互认证的会话密钥就是由KDC分发的，下面详细讲解KDC分发会话密钥的过程。 分发会话密钥过程1、首先客户端向KDC发送一个会话密钥申请。这个申请的内容可以简单概括为”我是某客户端，我需要个Session Key用于与某服务器通话“。 2、KDC在接收到这个请求的时候，生成一个会话密钥。为了保证这个会话密钥仅仅限于发送请求的客户端和它希望访问的服务器知道，KDC会为这个会话密钥生成两个副本，分别被客户端和服务器使用。然后从账户数据库中提取客户端和服务器的主密钥分别对这两个副本进行对称加密。对于服务器，与会话密钥一起被加密的还包含关于客户端的一些信息，以便对发起连接请求的客户端进行身份认证。 注意：KDC不是直接把这两个会话密钥副本分发客户端和服务器的，因为如果这样做，对于服务器来说会 出现下面两个问题。由于一个服务器会面对若干不同的客户端，而每个客户端都具有一个不同的Session Key。那么服务器就会为所有的客户端维护这样一个会话密钥的列表，这样对服务器来说工作量就非常 大了。由于网络传输的不确定性，可能会出现这样一种情况：客户端很快获得会话密钥用于副本，并将 这个会话密钥作为凭据随同访问请求发送到服务器，但是用于服务器的会话密钥却还没有收到，并且很 有可能这个会话密钥永远也到不了服务器端，这样客户端将永远得不到认证。为了解决这个问题， Kerberos将这两个被加密的副本一同发送给客户端，属于服务器的那份由客户端发送给服务器。因为 这两个会话密钥副本分别是由客户端和服务器端的主密钥加密的，所以不用担心安全问题。 3、通过上面的过程，客户端实际上获得了两组信息：一个是通过自己主密钥加密的会话密钥；另一个是被Server的主密钥加密的数据包，包含会话密钥和关于自己的一些确认信息。在这个基础上，我们再来看看服务器是如何对客户端进行认证的。 4、客户端通过用自己的主密钥对KDC加密的会话密钥进行解密从而获得会话密钥，随后创建认证符（Authenticator，包括客户端信息和时间戳（Timestamp）），并用会话密钥对其加密。最后连同从KDC获得的、被服务器的主密钥加密过的数据包（客户端信息和会话密钥）一并发送到服务器端。我们把通过服务器的主密钥加密过的数据包称为服务票证（Session Ticket）。 5、当服务器接收到这两组数据后，先使用它自己的主密钥对服务票证进行解密，从而获得会话密钥。随后使用该会话密钥解密认证符，通过比较由客户端发送来的认证符中的客户端信息（Client Info）和服务票证中的客户端信息实现对客户端身份的验证。 双方进行了身份认证的同时也获得了会话密钥，那么双方可以进行会话了。 流程图如下： 客户端简称为Alice，服务端简称为Bob 总结参考文章]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>KDC</tag>
        <tag>密钥</tag>
      </tags>
  </entry>
</search>
