<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[决策树分类算法]]></title>
    <url>%2F%E5%86%B3%E7%AD%96%E6%A0%91%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[概念讲解分类与聚类 在具体讲解分类和聚类算法之前，我们先了解一下什么是分类，什么是聚类，以及它们常用的算法有哪些。 1. Classification(分类)，对于一个classifier，通常需要你告诉它这个东西被分为某某类这样一些例子，理想情况下，一个classifier会从它得到的训练集中进行“学习”，从而具备对未知数据进行分类的能力，这种提供数据训练的过程一般叫做“Supervised Learning（监督学习）”。 2. Clustering（聚类），简单的说就是把相似的东西分到一组，聚类的时候，我们不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。因此一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此clustering通常不需要使用训练数据进行学习，这种叫做“Unsupervised Learning（无监督学习）” 分类：简单地说，就是根据文本的特征或属性，划分到已有的类别中。也就是说，这些类别是已知的，通过对已知分类的数据进行训练和学习，找到这些不同类的特征，再对未分类的数据进行分类。常用算法：决策树、朴素贝叶斯，支持向量机，神经网络，k-最近邻，模糊分类 聚类：简单地说，就是你压根不知道数据会分为几类，通过聚类分析将数据或者说用户聚合成几个群体，那就是聚类了。聚类不需要对数据进行训练和学习，一个聚类算法只要知道如何计算相似度就可以工作了。 什么是决策树所谓决策树，就是一种树，一种依托于策略抉择而建立的树。 机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 从数据产生决策树的机器学习技术叫做决策树学习, 通俗点说就是决策树，说白了，这是一种依托于分类、训练上的预测树，根据已知预测、归类未来。 来理论的太过抽象，下面举一个浅显易懂的例子： 套用俗语，决策树分类的思想类似于找对象。现想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话： 女儿：多大年纪了？ 母亲：26。 女儿：长的帅不帅？ 母亲：挺帅的。 女儿：收入高不？ 母亲：不算很高，中等情况。 女儿：是公务员不？ 母亲：是，在税务局上班呢。 女儿：那好，我去见见。 这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑： 决策树是一种十分常用的分类方法，需要监管学习，监管学习就是给出一堆样本，每个样本都有一组属性和一个分类结果，也就是分类结果已知，那么通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。 相关算法决策树学习之ID3算法从信息论知识中我们知道期望信息越小，信息增益越大，从而纯度越高。ID3的算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益（后面会讲解什么是信息增益）最大的属性进行分裂 所以ID3的算法思想便是： 自顶向下的贪婪搜索遍历可能的决策树空间构造决策树(此方法是ID3算法和C4.5算法的基础)； 从“哪一个属性将在树的根节点被测试”开始； 使用统计测试来确定每一个实例属性单独分类训练样例的能力，分类能力最好的属性作为树的根结点测试（如何定义或者评判一个属性是分类能力最好的呢？这便是下文将要介绍的信息增益 or 信息增益率，这里要说的是信息增益和信息增益率是不同的,ID3基于信息增益来选择最好的属性，而接下来介绍的C4.5则是基于增益率来进行选择，这也是它进步的地方）。 然后为根结点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支（也就是说，样例的该属性值对应的分支）之下。 重复这个过程，用每个分支结点关联的训练样例来选取在该点被测试的最佳属性。 这形成了对合格决策树的贪婪搜索，也就是算法从不回溯重新考虑以前的选择。 实例讲解分析回归决策树的基本知识，构建一个决策树主要有以下三个重要问题： 数据是怎么分裂的 如何选择分类的属性 什么时候停止分裂 从上述三个问题和ID3算法思想出发， 以一个实际例子对ID3算法进行讲解。 问题描述：我们统计了14天的气象数据(指标包括outlook，temperature，humidity，windy)，并已知这些天气是否打球(play)。如果给出新一天的气象指标据:sunny,cool,high,TRUE，判断一下会不会去打球。 table 1 outlook temperature humidity windy play sunny hot high FALSE no sunny hot high TRUE no overcast hot high FALSE yes rainy mild high FALSE yes rainy cool normal FALSE yes rainy cool normal TRUE no overcast cool normal TRUE yes sunny mild high FALSE no sunny cool normal FALSE yes rainy mild normal FALSE yes sunny mild normal TRUE yes overcast mild high TRUE yes overcast hot normal FALSE yes rainy mild high TRUE no 现在我们用ID3算法来讲解：预备知识讲解：1. 信息熵 2. 信息增益 决策树决策决策树的形式类似于“如果天气怎么样，去玩；否则，怎么着怎么着”的树形分叉。那么问题是用哪个属性（即变量，如天气、温度、湿度和风力）最适合充当这颗树的根节点，在它上面没有其他节点，其他的属性都是它的后续节点。 那么借用上面所述的能够衡量一个属性区分以上数据样本的能力的“信息增益”（Information Gain）理论。 如果一个属性的信息增益量越大，这个属性作为一棵树的根节点就能使这棵树更简洁，比如说一棵树可以这么读成，如果风力弱，就去玩；风力强，再按天气、温度等分情况讨论，此时用风力作为这棵树的根节点就很有价值。如果说，风力弱，再又天气晴朗，就去玩；如果风力强，再又怎么怎么分情况讨论，这棵树相比就不够简洁了。 用熵来计算信息增益，如上图例子： 1. 计算分类系统的熵 类别是 "是否出去玩"。取值为yes的记录有9个，取值为no的有5个，即说这个样本里有9个正例，5个负例，记为S(9+,5-)，S是样本的意思(Sample)。那么P(c1) = 9/14, P(c2) = 5/14 这里熵记为Entropy(S),计算公式为： $$ Entropy(S)= -\frac{9}{14}*log_2{\frac{9}{14}} - \frac{5}{14}*log_2{\frac{5}{14}} $$ 2. 分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益 我们来计算Wind的信息增益: 当Wind固定为Weak时：记录有8条，其中正例6个，负例2个；同样，取值为Strong的记录6个，正例负例个3个。我们可以计算相应的熵为： $$ Entropy(Weak)= -\frac{6}{8}*log_2{\frac{6}{8}} - \frac{2}{8}*log_2{\frac{2}{8}} $$ $$ Entropy(Strong)= -\frac{3}{6}*log_2{\frac{3}{6}} - \frac{3}{6}*log_2{\frac{3}{6}} $$ 现在就可以计算出相应的信息增益了： $$ Gain(Wind)=Entropy(S)-\frac{8}{14}*Entropy(Weak)-\frac{6}{14}*Entropy(Strong)$$ $$ =0.940-\frac{8}{14}*0.811-\frac{6}{14}*1.0=0.048 \qquad \quad $$ 这个公式的奥秘在于，8/14是属性Wind取值为Weak的个数占总记录的比例，同样6/14是其取值为Strong的记录个数与总记录数之比。 同理，如果以Humidity作为根节点： $ Entropy(High)=0.985 $;\quad $ Entropy(Normal)=0.592$ $$ Gain(Humidity)=0.940-\frac{7}{14}*Entropy(High)-\frac{7}{14}*Entropy(Normal)=0.151 $$ 以Outlook作为根节点： $ Entropy(Sunny)=0.971 ; \quad Entropy(Overcast)=0.0 ; \quad Entropy(Rain)=0.971 $ $$ Gain(Outlook)=0.940-\frac{5}{14}*Entropy(Sunny)-\frac{4}{14}*Entropy(Overcast)-\frac{5}{14}*Entropy(Rain)=0.247 $$ 以Temperature作为根节点： $ Entropy(Cool)=0.811 ; \quad Entropy(Hot)=1.0 ; \quad Entropy(Mild)=0.918 $ $$ Gain(Temperature)=0.940-\frac{4}{14}*Entropy(Cool)-\frac{4}{14}*Entropy(Hot)-\frac{6}{14}*Entropy(Mild)=0.029 $$ 这样我们就得到了以上四个属性相应的信息增益值： $ Gain(Wind)=0.048 ；\quad Gain(Humidity)=0.151 ；$ $ Gain(Outlook)=0.247 ；\quad Gain(Temperature)=0.029 $ 最后按照信息增益最大的原则选Outlook为根节点。子节点重复上面的步骤。这颗树可以是这样的，它读起来就跟你认为的那样： ID3优点是理论清晰、方法简单、学习能力较强，但也存在一些缺点： （1）只能处理分类属性的数据，不能处理连续的数据； （2）划分过程会由于子集规模过小而造成统计特征不充分而停止； （3）ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是 倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。 C4.5算法讲解 这一个算法，就不讲解了，有一篇博客讲的特别好，推荐看一下。 C4.5算法讲解博客地址： 总结 因为最近要进行数据挖掘的课程考试，所以在复习（其实是预习...）时，进行总结。我是观看了几篇博客讲解后进行总结的，主要是为了记录我学到的，后面可以进行回头查阅，如能帮助到其他人，那再好不过了。 参考文献 分类算法之决策树ID3详解 决策树分类算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 配置1--markdown文章常用功能]]></title>
    <url>%2FHexo-%E9%85%8D%E7%BD%AE1-markdown%E6%96%87%E7%AB%A0%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[文本居中引用 此标签将生成一个带上下分割线的引用，同时引用内文本将自动居中。 文本居中时，多行文本若长度不等，视觉上会显得不对称，因此建议在引用单行文本的场景下使用。 例如作为文章开篇引用 或者 结束语之前的总结引用。 使用方式 HTML方式：使用这种方式时，给img添加属性class=&quot;blockquote-center&quot;即可。 标签方式：使用centerquote或者简写cq。 效果展示 代码如下： 12345678&lt;!-- 标签别名 --&gt;&#123;% cq %&#125; 人的一切痛苦，本质上都是对自己的无能的愤怒 &lt;font color=black size=4 face=&quot;黑体&quot;&gt;冯相如 &lt;/font&gt;&#123;% endcq %&#125; 这句话与签名者姓名（冯相如）中间间隔的行数越多（回车越多），则最后效果中间间隔就会越大。 至于签名者姓名，你可以用markdown中的字体大小颜色进行设置，如： 123456&lt;font face=&quot;黑体&quot;&gt;我是黑体字&lt;/font&gt;&lt;font face=&quot;微软雅黑&quot;&gt;我是微软雅黑&lt;/font&gt;&lt;font face=&quot;STCAIYUN&quot;&gt;我是华文彩云&lt;/font&gt;&lt;font color=#0099ff size=12 face=&quot;黑体&quot;&gt;黑体&lt;/font&gt;&lt;font color=#00ffff size=3&gt;null&lt;/font&gt;&lt;font color=gray size=5&gt;gray&lt;/font&gt; 但是不要误解，这句话不是我说的，我只不过是为了测试“标签签名”功能。 Bootstrap Callout使用方式1&#123;% note class_name %&#125; Content (md partial supported) &#123;% endnote %&#125; 其中，class_name可以是以下列表中的一个值： default primary success info warning danger 效果示例 对应代码如下； 12345&#123;% note class_name %&#125; Content of class_name &#123;% endnote %&#125;&#123;% note default %&#125; Content of default &#123;% endnote %&#125;&#123;% note primary %&#125; Content of primary &#123;% endnote %&#125; 突破容器宽度限制的图片当使用此标签引用图片时，图片将自动扩大 26%，并突破文章容器的宽度。 此标签使用于需要突出显示的图片, 图片的扩大与容器的偏差从视觉上提升图片的吸引力。 此标签有两种调用方式（详细参看底下示例）： 使用方式 HTML方式：使用这种方式时，为img添加属性 class=&quot;full-image&quot;即可。 标签方式：使用fullimage 或者简写fi，并传递图片地址、 alt 和 title 属性即可。 属性之间以逗号分隔。 效果示例 添加emoji表情使用方式我的是Mac电脑，所以直接用快捷键，control+command+space调出表情框，然后点击输入即可；对于其他电脑，应该也可以直接搜到复制到markdown就好了。 效果示例🤣😜🤨🤩😎🐸🚗❤️🇨🇳 总结我会持续更新，今天先写到这儿，有什么不对，请指正。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试]]></title>
    <url>%2F%E6%B5%8B%E8%AF%952%2F</url>
    <content type="text"><![CDATA[文本居中引用Any content (support inline tags too). 我是白色的字体，背景是色的~ fairest creatures beauty's rose bear his memory abundance lies sweet self too cruel tender churl mak'st waste in niggarding To eat the world's due, by the grave and thee First unique name 1First unique name 2First unique name 3This is Tab 1. This is Tab 2. This is Tab 3. ButtonUsage: text Alias: text Button with textText Text Text & Title Button with icon Button with text and iconText & Icon (buggy) Text & Icon (fixed width) Text & Large Icon Text & Large Icon & Title Button inside other tagText &amp; IconText Button margin Almost Over Test is finished. 参考文献：https://almostover.ru/2016-01/hexo-theme-next-test/ 组合应用 Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus hendrerit. Pellentesque aliquet nibh nec urna. In nisi neque, aliquet vel, dapibus id, mattis vel, nisi. Sed pretium, ligula sollicitudin laoreet viverra, tortor libero sodales leo, eget blandit nunc tortor eu nibh. Nullam mollis. Ut justo. Suspendisse potenti. Pellentesque fermentum dolor. Aliquam quam lectus, facilisis auctor, ultrices ut, elementum vulputate, nunc. Sed egestas, ante et vulputate volutpat, eros pede semper est, vitae luctus metus libero eu augue. Morbi purus libero, faucibus adipiscing, commodo quis, gravida id, est. Sed lectus. Praesent elementum hendrerit tortor. Sed semper lorem at felis. Vestibulum volutpat, lacus a ultrices sagittis, mi neque euismod dui, eu pulvinar nunc sapien ornare nisl. Phasellus pede arcu, dapibus eu, fermentum et, dapibus sed, urna. Morbi interdum mollis sapien. Sed ac risus. Phasellus lacinia, magna a ullamcorper laoreet, lectus arcu pulvinar risus, vitae facilisis libero dolor a purus. Sed vel lacus. Mauris nibh felis, adipiscing varius, adipiscing in, lacinia vel, tellus. Suspendisse ac urna. Etiam pellentesque mauris ut lectus. Nunc tellus ante, mattis eget, gravida vitae, ultricies ac, leo. Integer leo pede, ornare a, lacinia eu, vulputate vel, nisl. 代码插入java 文件所在位置： \blog 12345678910111213141516/** * @author John Smith &lt;john.smith@example.com&gt;*/package l2f.gameserver.model;public abstract class L2Char extends L2Object &#123; public static final Short ERROR = 0x0001; public void moveTo(int x, int y, int z) &#123; _ai = null; log("Should not be called"); if (1 &gt; 5) &#123; // wtf!? return; &#125; &#125;&#125; 主题自带样式note标签且不问结果如何，尽自己之所能，积极的面对。 default primary Any content (support inline tags too). info warning danger danger no-icon 引用样式 内容 内容 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 且不问结果如何，尽自己之所能，积极的面对。 警告系列 本文旨在介绍样式的使用规则。 本文旨在介绍样式的使用规则。 本文旨在介绍样式的使用规则。 本文旨在介绍样式的使用规则。 Usage: 百度Alias: text url title 主题自带样式 FontAwesome &lt;/i&gt; 支持 MarkdownHexo 支持 GitHub Flavored Markdown 的所有功能，甚至可以整合 Octopress 的大多数插件。 &lt;/i&gt; 一件部署只需一条指令即可部署到 Github Pages，或其他网站。 &lt;/i&gt; 丰富的插件Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade，CoffeeScript。 采用的是 Font Awesome 的图标，下面给出一些简单的使用例子，更多请查看官网的使用示例。 铅笔 上传 下载 下载 下载变大 33% 下载两倍大 增加下载图标 Download Now 视频连接 YouTube 添加emoji😀😅 🙏 文字背景块样式1. 啦啦啦啦 啦啦啦啦 啦啦啦啦 啦啦啦啦 插入PDF文档以及图片插入PDF文档：将相应的PDF文档放在与博客标题同名的文件夹内，然后再按照如下方式进行插入。 // 在/hexo/blog/public这个文件夹下找到你的博客同名的文件夹，然后将该文件放进去就可以在网页上访问了 点我，这里是PDF文档 表格表头表格的表头使用标签进行定义。大多数浏览器会把表头显示为粗体居中的文本： Header 1 Header 2 row 1, cell 1 row 1, cell 2 row 2, cell 1 row 2, cell 2]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试博客照片墙功能]]></title>
    <url>%2FphotoPostName%2F</url>
    <content type="text"></content>
      <categories>
        <category>相册</category>
      </categories>
      <tags>
        <tag>照片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow中的维度和shape的理解]]></title>
    <url>%2FTensorFlow%E4%B8%AD%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8Cshape%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[TensorFlow 的名字已经说明了它最重要的两个概念—Tensor和Flow。Tensor就是张量，Flow翻译成中文就是“流”，它直观的表达了张量之间通过计算相互转化的过程。 Tensor在TensorFlow中是N维矩阵，所以涉及到Tensor的方法，也都是矩阵的处理。由于十多位，在TensorFlow中Tensor的流动过程就涉及到了升维降维。 该篇通过讲解Tensor张量中的shape的理解和一些接口的使用，来体会Tensor的维度概念。 理解TensorFlow的shape TensorFlow和python中的numpy库一样，读shape时应该从外向内读。 1[[1,2,3],[4,5,6]] 和 12[[1,2,3], [4,5,6]] 是一样的，都是2行三列(shape(2,3))。 这该怎么理解呢？首先拿掉最外层的中括号，变成了[1,2,3],[4,5,6]这两个元素，每个元素（如[1,2,3]）拿掉中括号后，剩下1，2，3这三个元素，所以shape=[2,3]。 也就是说shape是从外向内读，然后记录下每一层（即每一个中括号）的元素个数。 比如，shape=[1,1,2]，则表示的数据应该是这样的：[[[a,b]]]. 什么是维度？什么是轴（axis）？什么是维度？维基百科说： 维度，又称维数，是数学中独立参数的数目。在物理学和哲学的领域内，指独立的时空坐标的数目。 0维是一个点，没有长度。1维是线，只有长度。2维是一个平面，是由长度和宽度（或曲线）形成面积。3维是2维加上高度形成的“体积面”。虽然一般人习惯了整数维，但在分形中维度不一定是整数，可能会是一个非整的有理数或者无理数。 看上面的话，可能很费解，我也是！那么就接着往下看吧，会越来越简单的。 总结：维度是用来索引一个多维数组中某个具体数所需要最少的坐标数量。 这句话很有深度，但是你多读几遍，加上下面的例子，你肯定会有自己的理解。 不同的维度的形式如下： 0维，又称0维张量，数字，标量：1 1维，又称1维张量，数组，vector：[1,2,3] 2维，，又称2维张量，矩阵，二维数组：[[1,2],[3,4]] 3维，又称3维张量，立方，三维数组：[[[1,2],[3,4]],[[5,6],[7,8]]] n维，你应该可以自己总结出来了吧！ 再多的维只不过是把上一个维度当做自己的元素 所以看一个张量是几维的，就看最深的数字外有几个中括号！ 什么是axis axis 是多维数组每个维度的坐标 也就是说，对于[ [[1,2], [3,4]], [[5,6], [7,8]] ]这个3维情况，[[1,2],[[5,6]], [[3,4], [7,8]]这两个矩阵的axis是0，[1,2]，[3,4]，[5,6]，[7,8]这4个数组的axis是1，而1，2，3，4，5，6，7，8这8个数的axis是2。 Tensor维度理解 这里通过tf.reduce_mean来理解一下维度 tf.reduce_meanreduce_mean( input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None ) 计算Tensor各个维度元素的均值，这个方法是通过输入参数axis的维度上减少输入input_tensor的维度。 举个例子： x = tf.constant([1.,1.],[2.,2.]) tf.reduce_mean(x) # 1.5 tf.reduce_mean(x,0) # [1.5, 1.5] tf.reduce_mean(x,1) # [1., 2.] x 是 二维数组： 当axis参数取默认值时，计算整个数组的均值：(1.+1.+2.+2.)/4 = 1.5 当axis取0，意味着对列取均值：[1.5, 1.5] 当axis去1，意味着对行取均值：[1.0, 2.0] 在换一个 3*3 的矩阵： sess = tf.Session() x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) print(sess.run(x)) print(sess.run(tf.reduce_mean(x))) print(sess.run(tf.reduce_mean(x, 0))) print(sess.run(tf.reduce_mean(x, 1))) 输出结果为： [[ 1. 2. 3.] [ 4. 5. 6.] [ 7. 8. 9.]] 5.0 [ 4. 5. 6.] [ 2. 5. 8.] 如果我再加一维会是怎么样计算？ sess = tf.Session() x = tf.constant([[[1., 1.], [2., 2.]], [[3., 3.], [4., 4.]]]) print(sess.run(x)) print(sess.run(tf.reduce_mean(x))) print(sess.run(tf.reduce_mean(x, 0))) print(sess.run(tf.reduce_mean(x, 1))) print(sess.run(tf.reduce_mean(x, 2))) 我给输入的Tensor是三维数组： [[[ 1. 1.] [ 2. 2.]] [[ 3. 3.] [ 4. 4.]]] 推测一下，前面的二维经过处理都变成了一维的，也就是经过了一次降维，那么现在的三维或许应该变成了二维。但是现在多了一维，应该从哪个方向做计算呢？首先看输出结果： 2.5 # x [[ 2. 2.] # x,0 [ 3. 3.]] [[ 1.5 1.5] # x,1 [ 3.5 3.5]] [[ 1. 2.] # x,2 [ 3. 4.]] 发现： 当axis参数取默认值时，依然是计算整个数组的均值：(float)(1+2+3+4+1+2+3+4)/8=2.5 当axis取0，计算方式是： [[(1+3)/2, (1+3)/2], [(2+4)/2, (2+4)/2]] 当axis取1，计算方式是： [[(1+2)/2, (1+2)/2], [(3+4)/2, (3+4)/2]] 当axis取2，计算方式是： [[(1+1)/2, (2+2)/2], [(3+3)/2, (4+4)/2]] 总结： 规律： 对于k维的， tf.reduce_mean(x,axis=k-1)的结果是对最里面的一维所有元素求和取平均 tf.reduce_mean(x,axis=k-2)的结果是对倒数第二层里的向量对应的元素进行求和取平均 tf.reduce_mean(x,axis=k-3)的结果是对倒数第三层的每个向量对应元素进行求和取平均 参考链接 附图理解： 在TensorFlow 1.0版本中，reduction_indices被改为了axis，在所有的reduce_xxx系列操作中，都有axis也就是reduction_indices这个参数。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分式方程应用各个类型案例讲解]]></title>
    <url>%2F%E5%88%86%E5%BC%8F%E6%96%B9%E7%A8%8B%E5%BA%94%E7%94%A8%E9%A2%98%E5%88%86%E7%B1%BB%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、【行程中的应用性问题】例1 甲、乙两个车站相距96千米，快车和慢车同时从甲站开出，1小时后快车在慢车前12千米，快车比慢车早40分钟到达乙站，快车和慢车的速度各是多少？ === 分析：等量关系是什么？ —- 慢车用时 = 快车用时 + $\frac{40}{60}$ 所行距离 速度 时间 快车 96 km x km/h 慢车 96 km x-12 km/h 解：设快车速度为 x km/h，则慢车的速度为 （x - 12）km/h 所以： \frac{96}{x-12} = \frac{96}{x} + \frac{40}{60}解得：x = 48 km/h , x - 12 = 36 km/h 答：快车速度为 48 km/h , 慢车速度为 36 km/h。 例2 甲、乙两地相距828km，一列普通快车与一列直达快车都由甲地开往乙地，直达快车的平均速度是普通快车平均速度的1.5倍．直达快车比普通快车晚出发2h，比普通快车早4h到达乙地，求两车的平均速度． == 解：设普通快车车的平均速度为 x km／h，则直达快车的平均速度为 1.5x km／h，依题意，得： \frac{828-6x}{x} = \frac{828}{1.5x}解得：x = 46. 经检验，x = 46 是方程的根，且符合题意。 ∴ x = 46，1.5x = 69 答：普通快车车的平均速度为46km／h，直达快车的平均速度为69km／h。 例3 A、B两地相距87千米，甲骑自行车从A地出发向B地驶去，经过30分钟后，乙骑自行车由B地出发，用每小时比甲快4千米的速度向A地驶来，两人在距离B地45千米C处相遇，求甲乙的速度。 分析： 所行距离 速度 时间 甲 87-45 km x km/h 乙 45 km x+4 km/h 等量关系：：甲用时间=乙用时间+ $\frac{30}{60}$ （小时） 解：设甲的速度为 x km/h ，则乙的速度为 (x+4) km/h 所以， \frac{87-45}{x} = \frac{45}{x+4} + \frac{30}{60}解得：$x{1} = -24$（不符合题意，舍去）$x{2} = 14 km/h$，所以乙的速度为 18 km/h 答：……. 例4 某客车从甲地到乙地走全长480Km的高速公路，从乙地到甲地走全长600Km的普通公路。又知在高速公路上行驶的平均速度比在普通公路上快45Km，由高速公路从甲地到乙地所需的时间是由普通公路从乙地到甲地所需时间的一半，求该客车由高速公路从甲地到乙地所需要的时间。 二、【工程类应用性问题】例1 甲乙两个工程队合作一项工程，两队合作2天后，由乙队单独做1天就完成了全部工程。已知乙队单独做所需天数是甲队单独做所需天数的 $\frac{3}{2}$ 倍，问甲乙单独做各需多少天？ 解：设甲单独做需要 x 天，则乙单独做需要 $\frac{3x}{2}$天，则： \frac{2}{x} + \frac{3}{\frac{3x}{2}} = 1解得：x = 4，则$\frac{3x}{2} = 6$ 答：甲单独做需要4天，乙单独做需要6天。 分析：这个等式的意思是什么？我们知道设 甲单独做需要 x 天，则按照题意，甲在这个项目中一共做了2天，则这两天完成了这个项目的百分之多少呢？ 不就是 $\frac{2}{x}$；则乙在这个项目中一共做了 3天，则这三天做了整个项目的百分之多少呢？ 不就是 $\frac{3}{\frac{3x}{2}}$。 所以加起来就是把这个项目完成了百分之百，即1. 所以等式为： \frac{2}{x} + \frac{3}{\frac{3x}{2}} = 1例2 甲、乙两个施工队共同完成某居民小区绿化改造工程，乙队先单独做2天后，再由两队合作10天就能完成全部工程．已知乙队单独完成此项工程所需天数是甲队单独完成此项工程所需天数的$\frac{4}{5}$，求甲、乙两个施工队单独完成此项工程各需多少天？ 解：设甲施工队单独完成此项工程需x天，则乙施工队单独完成此项工程需$\frac{4x}{5}$天，根据题意，得: \frac{10}{x}＋\frac{12}{\frac{4x}{5}}＝1解这个方程，得x＝25，经检验，x＝25是所列方程的根，所以 $\frac{4x}{5} = 20$ 答：甲单独完成需要25天，乙单独完成需要20天。 三、【营销类应用性问题】 营销类应用性问题，涉及 进货价、售货价、利润率、单价、混合价、赢利、亏损 等概念，要结合实际问题对它们表述的意义有所了解，同时，要掌握好基本公式，巧妙建立关系式．随着市场经济体制的建立，这类问题具有较强的时代气息，因而成为中考常考不衰的热点问题． 例1 某校办工厂将总价值为2000元的甲种原料与总价值为4800元的乙种原料混合后，其平均价比原甲种原料每千克少3元，比乙种原料每千克多1元，问混合后的单价每千克是多少元？ 解：设混合后的单价为每千克 x 元，则甲种原料的单价为每千克 (x+3) 元，混合后的总价值为 $(2000＋4800)$ 元，混合后的重量为 $\frac{2000+4800}{x}$ 斤，甲种原料的重量为 $\frac{2000}{x+3}$ ，乙种原料的重量为 $\frac{4800}{x-1}$，依题意，得： \frac{2000}{x+3} + \frac{4800}{x-1} = \frac{2000+4800}{x}解得：x = 17，经检验，x = 17 是原方程的根，所以 x=17．即混合后单价每千克为17元。 答：…….. 例2 某商场销售某种商品，一月份销售了若干件，共获得利润30000元;二月份把这种商品的单价降低了 0.4元，但是销售量比一月份增加了5000件，从而获得利润比一月份多2000元，调价前每件商品的利润为多少元？ == 分析： 可以列出三个等量关系 1．2月份销售量一1月份销售量=5000 2．2月份销售量×2月份利润=2月份总利润 3．1月份利润一2月份利润=0.4 解：由题意知，根据 二月所获利润 = 30000 + 2000 等式列方程，设调价前每件商品利润为 x 元，则调价后即2月份每件商品利润为 （x-0.4）元，得： (x-0.4)(\frac{30000}{x} + 5000) = 30000 + 2000化简得：$5x^2 -4x -12 = 0$，所以$(x-2)(5x+6) = 0$ 解得 x=2 或 x=-1.2（舍去） 答：调价前每件商品利润是2元。 例三 某商厦进货员预测一种应季衬衫能畅销市场，就用8万元购进这种衬衫，面市后果然供不应求，商厦又用17.6万元购进了第二批这种衬衫，所购数量是第一批购进量的2倍，但单价贵了4元，商厦销售这种衬衫时每件定价都是58元，最后剩下的150件按八折销售，很快售完，在这两笔生意中，商厦共赢利多少元。 解：由题意知，设商场第一次购进 x 件衬衫，则第二次购进 2x 件， \frac{80000}{x} = \frac{176000}{2x} -4解得：x = 2000，经检验，x=2000是这个方程的根，∴ 2x=4000。 商场利润 = 两次总共所买获得总钱 - 两次进购所花总钱。 (2000+4000-150)*58+58*0.8*150 - 80000 - 176000 = 90260 （元）答：在这两笔生意中，商场总共盈利90260元。 例4 一个批发兼零售的文具店规定：凡一次购买铅笔300枝以上，（不包括300枝），可以按批发价付款，购买300枝以下，（包括300枝）只能按零售价付款。小明来该店购买铅笔，如果给八年级学生每人购买1枝，那么只能按零售价付款，需用120元，如果多购买60枝，那么可以按批发价付款，同样需要120元， （1）这个八年级的学生总数在什么范围内？ （2）若按批发价购买6枝与按零售价购买5枝的款相同，那么这个学校八年级学生有多少人？ 解：（1）由题意知，设八年级总人数为 x 人，每人购买一支，只能按零售价，所以 x≤300，如果多购买60支，则可按批发价，则 x + 60 &gt; 300 所以， 240 &lt; x ≤ 300 （2）根据按批发价购买6支与零售价购买5支的所需钱相同，列等式求解： \frac{120}{x}*5 = \frac{120}{x+60}*6解得，x = 300，经检验，x=300是原方程的根。 答：这个学校八年级人数有300人。 例5 某种商品价格，每千克上涨1/3，上回用了15元，而这次则是30元，已知这次比上回多买5千克，求这次的价格。 例6 小明和同学一起去书店买书，他们先用15元买了一种科普书，又用15元买了一种文学书，科普书的价格比文学书的价格高出一半，因此他们买的文学书比科普书多一本，这种科普和文学书的价格各是多少 四、【轮船顺逆水应用问题】例1 轮船顺流、逆流各走48千米，共需5小时，如果水流速度是4千米/小时，求轮船在静水中的速度。 === 分析：顺流速度=轮船在静水中的速度+水流的速度 逆流速度=轮船在静水中的速度-水流的速度 解：设轮船在净水中的速度为 x km/h，得： \frac{48}{x+4}+\frac{48}{x-4} = 5解得：x = 20 或 x=-0.8（舍去） 答：船在静水中速度为 20 km/h。 例2 例2 轮船在顺水中航行30千米的时间与在逆水中航行20千米所用的时间相等，已知水流速度为2千米／时，求船在静水中的速度。（10） 例3 轮船顺水航行80千米所需要的时间和逆水航行60千米所用的时间相同。已知水流的速度是3千米/时，求轮船在静水中的速度。 例4 已知一个汽船在顺流中航行46千米和逆流中航行34千米共用去的时间正好等于它在静水中航行80千米用去的时间并且水流的速度是每小时2千米求汽船在静水中的速度。 例5 9、一艘轮船在静水中的最大航速为30千米/时，它沿江以最大航速顺流航行100千米所用的时间与以最大航速逆流航行60千米所用的时间相等，问：江水的流速为多少？设江水的流速为x千米/时，则可列方程? 五、【其他应用性问题】例1 例1 要在15%的盐水40千克中加入多少盐才能使盐水的浓度变为20%． 分析：设加入盐千克．浓度问题的基本关系是：$\frac{溶质}{溶液}=浓度$ 解：设应加入盐 x 千克，依题意，得 \frac{40*15\%+x}{40+x} = 20\%解得：x=2.5，经检验，x=2.5是原方程的根 答：需要加入2.5kg的盐。 例2 某校招生录取时，为了防止数据输入出错，2640名学生的成绩数据分别由两位程序操作员各向计算机输入一遍，然后让计算机比较两人的输入是否一致，已知甲的输入速度是乙的2倍，结果甲比乙少用2小时输完。问这两个操作员每分钟各能输入多少名学生的成绩？ 例3 供电局的电力维修工要到30千米远的郊区进行电力抢修，技术工人骑摩托车先走，15分钟后，抢修车装载这所需材料出发，结果他们同时到达，，已知抢修的速度是摩托车的1.5倍。求这两种车的速度。]]></content>
      <categories>
        <category>家教</category>
      </categories>
      <tags>
        <tag>曹晨</tag>
        <tag>家教</tag>
        <tag>案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于网格的运动统计，用于快速、超鲁棒性的匹配]]></title>
    <url>%2FGMS-Grid-base%20Motion%20Statistics%20for%20Fast%EF%BC%8CUltra-robust%20Feature%20Correspondence%2F</url>
    <content type="text"><![CDATA[前序今天分为两个部分讲解： 关于Feature Matching 的介绍，可能有些人对feature matching本身不太了解。 最近几年关于关于feature matching的文章，主要有三篇。 前两篇文章主要的工作是：把feature matching做的特别的robust，之前的一些作法可能能基本的实现基本的match，但是因为match的质量不高，导致很多应用是不能用的，比如三维重构。但是经过前两篇文章，就可以实现之前不能实现的了，提升了鲁棒性，具有了很大的提高。该篇文章主要是对前两篇文章的优化，因为前两篇虽然最终效果很好，但是速度很慢。对于三维重构来说速度勉强可以，但是对于机器人导航、SLAM、无人机这些应用来说，速度就不够了，作者就对这个问题进行了时间的优化，最终达到的效果就是：性能和前两篇文章实现的相近，时间提升了很多，很快。希望能把这个算法应用到更多实时应用上。 Feature Matching Introduction 我们可以看一下这两张图，图中山峰是同一个山峰，但是是由人由两个角度进行拍照。所以我们如果能把两张图片中对应的点找来，找出来有什么用呢？我们知道上面的两幅图是由人从不同的角度拍照得来的，那么这两幅图中间是有一个几何关系，这个几何关系可以由一个方程来表示。那么每一个匹配点就相当于这个方程的解，如果我们有足够多的正确匹配的点，我们就可以用这些点来估算这个方程里面的参数，相当于我们知道了两张图片中对应的几何关系 Correct Correspondences的应用你把两张图片中相同的点匹配起来有什么用？ Geometry between 2 views，Geometry就是两张图片中的几何关系，这样我们便可以用来做很多事情：比如：Estimate Camera Pose Localization(SFM)、Tracking(SLAM):就是你能把一帧一帧图像之间的相对应的几何关系能算出来，如果很准的话，你就能画出来人运动的轨迹，就是一个人导航问题。 Similarity（Number of matches），根据Feature matching的多少评测两张图片的相似度。然后我们就可以用来做Image Retrieval(图像检索)、Object Recognition(目标识别)、Loop Closing(SLAM)…. 如何实现Feature Matching就是在两张Image中相同的这些部分找到，再把它们匹配起来。 首先是如何找到，包含两个步骤：Detection、Description。 Detection：首先找到的是角点，再者比较容易找的是边缘的点。 Description：对于这些每一个点计算出周围的特征，比如sift是用128维的数字去描述他的一个description。 matching的时候，只要用这个128维的数字计算跟哪个点的距离最小，就是最相似的 Geometry：你拿到这些点的匹配后，去做一个他们之间的相关的几何匹配，模拟几何关系，符合这个几何关系的就是正确匹配的点，不符合的就是错误的点。有了几何关系就可以坐后面的事情了。 LIFT：是用深度学习做的，他认为用深度学习得到的feature比手动提取的feature 更加有用 Matching Algorithms：CODE、RepMatch、GMS。因为Brute-Force和Approximate（FLANN）所获取的匹配点是杂乱的，大量的，这样你最后算几何关系就会很慢，效果还不好。我们这些Matching Algorithms算法是用来从Nearest-Nerighbor match中找出正确的匹配点，错的剔除去。 如何估计两张图片之间的几何关系一般用的是RANSAC-based方法。 RANSAC：Random Sample Consensus，它是根据一组包含异常数据的样本数据集，计算出数据的数据模型参数，得到有效样本数据的算法。 RANSAC也做了一下假设：给定一组（通常很小的）局内点（符合最优模型的点为局内点，不符合的定义为据外点），存在一个可以估计模型参数的过程而该模型能够解释或者适用于局内点 一个简单的例子就是从一组观测数据中找到合适的2维直线。假设检测到的数据如上图左图。简单的最小二乘法不能找出适应与局内点的直线，原因是最小二乘法尽量去适应包括局外点在内的所有的点。相反，RANSAC能够得出一个仅仅用局内点计算出模型，并且概率还足够高。但是RANSAC并不能保证结果一定正确，为了保证算法有足够搞得合理概率，我们必须小心的选择算法的参数。 在我们的Feature matching里面，它的RANSAC就不是一条直线了，而是两张图片中的几何关系。这里有两个普遍的几何关系： Fundamental Matrix（for 3D scenes）：Point to Line(weak, general)只能是点对线的匹配，就是给定第一张图片中的一个点，它会在第二张图片中画出这个点所在的线。能把一个点和一条线对应起来，这就是它们之间的几何关系，这个几何关系能够恢复出来相机的旋转和平移，用的方法主要是RANSAC方法 Homography (for 2D scenes)：Point to Point（strong，narrow range）只对2D场景有效，比如你拍一张墙的照片，然后对其中一个点，就能匹配到第二张图片的点。 Recent Robust MatchersCODE [1] 该算法解决的是 wide-baseline matching问题。 wide-baseline matching ？基线的本意是指立体视觉系统中量摄像机光心之间的距离。一句拍摄两幅图，像的视点位置可将对应点匹配问题分为宽基线和窄基线。宽基线一词用于匹配时，泛指两幅图像有明显不同的情况下匹配。产生这种情况的原因有可能为摄像机之间的位置相差很大，也有可能由于摄像机旋转或焦距的变化等因素产生。 每一幅图片中的其实是两幅图片，图片中的墙是相同的一个墙，只是由于拍摄的角度特别的大，传统的一些算法处理的不是很好，但是code算法可以，匹配的比较多，准确率也比较高 Idea 整体思想如上图，包含了三个回归模型，likelihood是第一个模型，affine motion 为第二第三个模型。这些回归模型就相当于分类器。 首先输入的是 Selected matches ，就是先挑一些比较好的match，把它们记录下来，去fit第一个模型，其实就相当于是一个分类器，然后用这个分类器把所有match做一个过滤，符合这个的留下来，不符合的剔除。 然后再用后面两个模型（即分类器）去过滤，最后都通过的才算最终正确的匹配点。 这三个回归是怎么做的？看下图（回归模型） 有一堆散的点，用这些数据去fit一个平面，这个平面上的你认为是对的，不在这个平面上，你认为是错误的。这样的regression总共有三步。 Likelihood train data：是我们选出来的那些好的match test data：是所有的的match features of a correspondence：把每一个match点定义为8位的向量，x,y是它的空间位置，dx，dy描述了它的motion，就是一对match点，左右各有一个点，用第二个点（第二幅图）的x2 减去 第一个点x1得到dx。T1-T4是一个矩阵的变换。我们看到的是点对点的匹配，其实是区域的匹配，这四个参数就是描述了左边的这个小区域到右边的对应的小区域的变换。 Label：我们开始初始化为都是1，认为都是对的。 Non-linear Optimization：描述了所有correspondence之间的相似性，相似性大的就是1，小的就是0 likelihood模型为什么有用，它就是用来区分连续的Motion和不连续的Motion，作者认为不连续的Motion一般就是错的。第一个模型就是找那些correspondened是连续的。第二个模型就是把它的精度提升了一下，把x和y都预测了一下。作者认为 (a) (b) 这样的图像中的点是正确的点，而像 (c) 这两个比较近的点的方向还差这么大，就认为是错的。第一个模型likelihood就是找到连续的motion，那些点的motion是连续的。第二个模型就是提升了它的精度，把x和y都测了一下。 结果code算法的input就是那个Nearest Neighbor match，它里面是乱七八糟，所有的match都有，而code就是把其中的对的找出来。 应用 主要是三维重构，你上面第二幅图是用商业软件做的，并不是很好，第三幅是用SFM做的，第四幅还是用SFM做的，但是它把SFM中的Feature matching替换成CODE算法得出的feature matching，效果就好很多。 Run time comparison 纵轴是时间表示，以 s 为单位，横轴是matches的数量，可以看到code运行的时间要比其他的好很多，但是对于slam ，机器人导航，一秒钟要读取几十帧，code这个算法的时间还是不够的。 RepMatch [2] 基于第一个算法之上，解决一个重复结构（repeated structure）的问题。它是用第一个算法的输出，然后在这个输出上再做一次优化，筛选一些对的点。 repeated structure到底是一个什么问题 比如 (a) 图中，我们拍了一栋楼的正面，侧面，反面等照片，然后进行SFM三维重构，发现结果它是给我化成了四个部分，但其实它是一栋楼，就是说三维重构并没有重构出这栋楼。原因：因为（a）图像中的楼的各个面的窗口都很像，你也不能确定正面的窗口和背面的窗口有什么不一样，所以导致一个问题：比如SFM就给你标出四个独立的模型，这明明是一栋楼，它合不起来。主要是因为他们之间错误的匹配太多了，你把楼的前面跟后面匹配在一起，它脑子就乱了，根本不知道这是个什么模型。 然后用了RepMatch，就能构造一个完整的模型，周围的每一个点是每一张相片拍照的位置，中间这一圈就是楼的俯视图，你他最后dense reconstruction的结果就是最右边的图 Repetitive Structure（怎么做到的构建成功的）idea:是做分类它是建立在CODE 输出结果之上的。它把输出结果分为了几类： 第一类：把结果放的严一些，然后结果数就很少。 GMS这篇文章主要是把前面两篇文章的主要思想掌握到，然后把它变快，能够把它应用到更多上面。 首先我们看一个小视频，上面的是作者的GMS算法，下面的是sift，我们可以发现，GMS的match首先在数量上就比sift的多很多，这样我们就能保证最后验证那个几何关系比较准，如果点比较少的话，你做RANSAC的话，就会不稳定，出错之类的。作者的算法比sift这个算法更快更好。 Motivation：为什么又要质量好又要速度快 最常用的算法可能就是Ratio Test，比较流行，比较快，但是不具备鲁棒性。当前的算法比如code等，效果很好，但是很慢。GMS特点就是：又快又好。 论文GMS的方法实际上是消除错误匹配的一种方案，比如可以替换ransac。算法执行的大致流程是：先执行任意一种特征点的检测和特征点的描述子计算，论文中采用的是ORB特征。然后执行暴力匹配BF，最后执行GMS以消除错误匹配。 Key IdeaTrue matches(green) are visually smooth while false matches(cyan) are not. 正确匹配的点我们用绿色的标注出来，错误匹配的点用青涩标注出来。首先我们可以看到正确匹配的点，举个例子，假设随便拿出来两个正确匹配的点（绿色标注的）记为1号点和2号点，1、2号点在另一幅图中对应匹配的点为1‘和2’。我们可以看到1号点对应的匹配点1‘与2号点对应的匹配点2’位置差不多，相似，整个过程是一个比较smooth的过程。而两个相近错误的点（青色的标注），对应的匹配点一个去了楼上，一个去了天上，这就是我们从观察上得到的信息。那么如何把这个观察的信息变成一个算法去验证这个点是对还是不对，这就是算法的核心。 首先我们说了一个正确的match看起来比较smooth，所以我们有没有办法把这些看起来比较smooth的match提取出来，我们就认为这些match是对的。根据贝叶斯上讲，既然正确的match看起来比较smooth，那么比较smooth的match应该也是正确的。 Key Idea：核心思想就是我们如何把match中比较smooth的match拿出来。 方法主要分为三部分： Motion Statistics：motion统计 Grid Framework：grid结构是用来加速的 Motion Kernels：也是用来加速的 后面两个部分都是为了实现第一个部分Motion统计，让它变得很快。 Motion StatisticsMotion Statistics Model首先看一下统计模型是怎么样的，对每一个match，比如说match xi，给他左边和右边各画一个圈，数一下圈中还有多少个其他的match 这个有什么道理：如果一个正确的match，它旁边还有正确的match的话，应该是支持它的。（就像之前的图中显示那样，对的，周围一般Motion是相似的）道理就是一个正确的match旁边可能有对的点在帮你，而对于一个错误的match，你的这个错误是随机发生的，你很难再找到一个点跟你犯一样的错误。所以这两个的概率相差非常大的，第一种正确的match这个圈里面总会有一些点来支持它，而错误的match 支持它的点可能有，但是会很少，这就是这个算法的模型。 这个图中Si就是表示统计值，这张图中Si = 2，Sj是下面那个，没有人支持他，所以是0.所以motion统计就是这样一件事情。 Si = 2；Sj = 0. 我们分析一下这个模型为什么对于判断“这个点是否正确”有效我们假设一个点的统计值 Si ， 它应该服从这个binomial distribution（二项分布）。就假设这个 xi 是正确的的话，那么用 pt 表示这个概率，这个概率是指：假设上图中左边的圈为A，右边的圈为B，那么如果这个点 xi 匹配是正确的话，它周围的点从A到B的可能性是多少，就是支持这个点 xi 的可能性是多少。对于这个 pf ，如果这个 xi 匹配是错的，那么它周围的点支持它的概率是多少。 我们用 fa 表示 A 这个圈中额外支持它的n个点中的一个点。然后去算这个概率，我们要给出一个事件和假设，如下图： 第一个事件表示：fa这个点匹配正确，它的概率为 t，这个概率跟这个算法有关，匹配的质量越高，那么这个概率当然要高 第二个事件表示：fa 这个点匹配错误，它的概率就是 1-t 第三个事件表示：fa 这个点 匹配到了 B 圈中的一个点（但不一定是正确的） 定义这三个事件后，我们给出一个假设：如果一个点它匹配错了，它的匹配还是从A到B的话，那么它的概率是多少 。我们知道如果是A中的一个点匹配错了，那有可能匹配到右边Image中的任何一个点，而它恰好匹配到B圈中的点，如果是随机分布的话，那么这个概率应该为 (m / M), m为B圈中的点的个数，M 为右边图中所有的点的个数。但是可能很多Image是这样的分布，但是有一些Image不是，我们为了弥补这个假设（随机分布），我们给它乘以一个 “贝塔”参数，当这个参数比较大的时候，这个假设就不那么成立了 。这个参数为1的时候，这个假设正好成立。 我们算一下这个概率，如下图： 因为m很小，所以 pf 是一个接近 0 的概率。而 pt 是一个接近 t 的数，因为 pf 是接近 0 的数 。所以这两个的差异很大，另外我们要怎么才能让这个差异变得更大？ 我们上面是在两张图中各画了一个小圈AB，这个小的区域中，他们的正确的点motion是连续的，通常一个Image中，不是只有这个一个小的区域中的motion是连续的，更大的区域应该也是连续的。 比如，之前那张狗的图片，我们只是画了狗的鼻子的一小块区域，但是其实鼻子旁边的区域也是连续的，当然我们不是画整张图片，只是比那个小圈更大一些，所以扩展到了 3*3的这样一个区域，相当于比之前的一个圈扩大了9倍，如下图： 我们之前只是画了a、b这两个圈，现在我们画了9个这样圈，并且统计这个 3 * 3 这样的区域中的 Si。这样便能增大这个distribution之间的差异，如下图： 因为是3*3，所以 K=9 ，这个Si的分布中 pt 和 pf 是跟之前的一样的，因为只有K变化了。所以计算一下均值和方差，如下图： 这个是后面要用到。 现在我们分析一下这个模型有什么用 Analysis Model Partionalbility 这个词是表示，对的点和错的点之间有多大的差距，看起来在这个模型中有多么不一样。P就是用来定义这两个分布（错的和正确的分布）的距离。P越大，表示这个错误的点和正确的点看起来越不一样，那么这个模型就越强。 Quantity-Quality equivalence 我们可以看到这个 P 是正比于这个 Kn 的，就是说你用越多的点在里面，那么这个模型就越强，错的点和对的点的区别就越大。 Relationship to Descriptors 看与这个descriptors的关系，这个descriptors就是 t ，比如你用sift特征点，假如这个sift特征特别强的话，那么这个P是趋近于无穷大的。这就能验证我们的算法是对的。，也比较合乎道理，你特征越强，你越好分割，所以 P 趋近于无穷大。 Experiments on real data该模型在Oxford Affine Dataset上进行评估。 在这里，我们运行SIFT匹配并根据基础事实将所有匹配标记为内点或外点。 我们计算每个匹配的一个小区域内的支持的个数。 纵轴表示了每个matching旁边画了两个小圈，然后数一数这个小圈中 正确的Inlier周围中到底有多少个点支持它，错的点到底有几个点支持它。 它有8的小的datasets，每个dataset里面有五对张照片，这五对照片匹配起来一对比一对难，就是越来越难，每个数据，是用8个datasets的数据做了平均得到的。 所以从上图可以看出：对于正确的点，就算是最难得时候，也有接近8个点支持它，而对于错误的点，始终没有超过1个点支持它。所以可以看出这个模型对于正确的点和错误的点的认识性还是很强的。 这个算法我们已经知道了，那么如何去加速实现它呢？ Grid Framework我们刚才是给每对匹配点画两个圈，一个点一个圈，这样的问题就是：你要想知道有多少个点这个圈内，那你必须要把所有的点过滤一遍。 所以现在我们的做法是不要给这个匹配点画圈，我们直接把image 打成网格，然后这个匹配点自然就会落在这个Image的网格中，那么它这个网格就代表他的这个小圈，就没有了两个小圈的匹配，而变成了两个网格的匹配，你只要数一下这个网格中有多少个点跟它一样就可以了。 只要你一开始把这些 Feature matching丢进这些网格中，你很容易数清楚。这样算法的时间复杂度就从O(n) -&gt; O(1). 所以会变得很快。 Motion Kernel因为我们之前已经知道用更多的region会产生更好的结果，因为我们进行统计的时候就变成了统计这九个小网格的正确点的数量就好了，就很容易能数清楚。 Empirical parameters（经验参数） 你用多少个网格？ 太细的话，统计不好 太粗的话，精度太低 经验上作者是说用 20 * 20 How to set the threshold？怎么设置阈值？就是认为计算出来的这个值（用 τ 表示）过了多少是对的，没过多少是错的。用的是一个平均值加一个方差，这是经验得到的，并没有什么道理可言。这个 α 参数一般我们设置为 0.6 。 Grid Motion Statistics Algorithm首先是给定这个 Correspondences（匹配点）、scale、rotation，然后产生这个 Kernel 和 Grid（20 * 20）。确定好了，去计算每一个点的统计值 Si ，然后去计算这个 τ，如果这个 Si &gt; τ，那么这个点就是对的。没过就是错的。 Full feature matching pipeline这是一个完整的流程，首先获取 feature 然后做 match，然后交给GMS，因为这个scale、rotation可能有多个值，你可以多试几遍，找到一个最好的值。 Run time这里是提取10000个点为例。这个值可以再Video上实时的，因为ORB和Nearest neighbor可以并行 Evaluation 我们测试了4个dataset，第一个是TUM。做slam的可能知道，下面是6个视频，然后从中提取出很多帧做测试，所以这个数据集上有3000多个照片。 0-30是表示两张图片的角度，就是baseline。至于在Strecha和VGG上，Ratio算法表现并不差，是因为这两个数据集太水了，大家是用来发paper的。像TUM这样真正的应用数据上来看，一般的算法根本就不太行了，质量会很差。 X轴是时间，用的是ms表示，Y轴是performance，红色的是作者的算法表现。可以看出GMS实现了结果上差不多，但是时间上快很多倍的效果。 我们从上图可以看出，第一幅图，是在平面上，GMS和SIFT比来看，SIFT经过RANSAC后，结果还是勉强可以的。就是说在平面上，这个sift加上RANSAC结果还是可以的。但是在3D中，sift匹配的全都是错误的点，但是GMS依然是表现很好。说明我们这个算法对于wide-baseline问题是真的解决了，不像sift只有在实验中（那些dataset上）表现的还可以接受，但是在现实中的dataset就不行了。 再就是解决了重复结构的问题，如下图： 之前我们是假设物体是静止的，但是这些猫狗是会动的，动了之后，你拍的图片用sift去匹配的话，如果有的匹配错，并且没有那些RANSAC方法去帮它修正的，那么你匹配对的就是对的，错的就是错的，没有办法去修改。而作者的算法是可以匹配正确的，根本不需要去修正。]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GMS</tag>
        <tag>CVPR 2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac常用快捷键教程]]></title>
    <url>%2FMac%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[用option快速输入一些特殊符号 option V： 输入 √ option /： 输入 ÷ option =： 输入 ≠ option &gt;： 输入 ≥ option &lt;： 输入 ≤ option K： 输入 ˚ option X： 输入 ≈ option =： 输入 ≠ option 2： 输入 ™ option R： 输入 ® option Y： 输入 ¥ option P： 输入 π option G： 输入 © option + shift + K：  通知中心开关 按住 option 点击桌面右上角通知中心，即可直接关闭或开启通知。不用在通知中心中打开了。 比如用 Mac 全屏看电影又不想被右上角的消息弹出打扰的话，暂时开启勿扰（关闭通知）就是了. WIFI状态按住 option 点击 Wi-Fi 图标即可查看 IP 地址、Wi-Fi 信号强度等等 Wi-Fi 的详细信息。 通用快捷键 command + M：最小化窗口 command + H：隐藏窗口 command + N：新建窗口 command + O：打开 command + Shift + S：另存为 command + P：打印 command + W：关闭 command + Q：退出 文件管理器 command + I：显示简介 command + F：搜索 command + delete：删除 command + Shift + delete：清空回收站 option + 方向键左：将光标移动到前一个单词 command + option + I：查看多个文件一共有多大 option + 方向键上：将光标移动到当前段落的开头 command + shift + 方向键左/右：可以选中一句话 command + 方向键左：将光标移动到句子的开头 command + 方向键上：将光标快速移动到整篇文本开头 浏览器 command + +\=：放大 command + --：缩小 command + T：新建一个选项卡 command + N：新建一个新窗口 command + R：刷新 command + F：搜索 command + W：关闭当前选项卡 command + D：将网页加入个人收藏 command + Z：恢复刚才关闭的网页 command + shift + B：显示或隐藏收藏栏 command + shift + N：打开一个全新的隐私浏览器窗口比较合适 space / space + shift：向下或向上滚动一整页 command + option + Q：退出 Safari，在下次打开的时候，会自动加载上次的所有窗口 option + 方向键上/方向键下：网页向上或向下滚动一整页 系统 command + option + esc：强制退出程序 command + shift + Y：生成便签 command + space：切换输入法 command：按住command即可用鼠标移动拖动顶部图标排列 control + command + space：快速调出emoji表情和各种特殊符号输入 shift + option + F1 / F2：可以以1/4格为单位对屏幕亮度调整，音量也是如此。 总结后续会继续补充。有什么遗漏的，可以留言。]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吵架]]></title>
    <url>%2F2018.4.27%2F</url>
    <content type="text"><![CDATA[今天是怎么了，或者说这几天是怎么了，天天惹你生气，心情很烦躁，我准备开始写日记，抒发一下自己的心情，要不我可能会被憋死的。 前序环顾四周和手机应用，竟然没有一个地方可以让我肆无忌惮的畅所欲言的发泄。在朋友圈、空间、微博你敢发吗，人长大了，总是要顾虑这顾虑那，我不想发在朋友圈后，让你再多想，我只是想找一个地方写写我的心情，要不我可能真的会被郁闷死的。 回忆中学初中时候，我个人的品性很差，现在想想那可能是最灰暗的三年了，没有知心朋友，脾气古怪，小心眼，自己一个人卑微的活着，也不知道打了多少架，现在想想，心中只是一阵痛。我只能说，有时候真的不是我的错，我还记得初中上学时，学校不让住校，我就只能在学校外跟同学合租房子，大约有5、6个人吧。其中有一个是邻班的班长，有天晚上，他就突然找我的茬。有人可能说，人家怎么会莫名其妙的找你的茬，但我可以摸着良心说，我真的没有做错什么。那天晚上，他跟我打了一架，我被揍了，哭着跑了出去，大晚上自己一个人走在马路上，一个人哭，最终我还是去给我妈妈打了个电话，最终的结果就是，我妈妈来了学校，他妈妈也来了学校，和解。后来我才知道，他为什么要跟我打架，因为他想让他一个好朋友住进来，把我踢出去，现在想起来我还是很痛恨他。 事情原委话扯远了，今天晚上也不知道为啥心情这么暴躁，刚开始和你打游戏时的心态可能就不对，最后因为我想用法师貂蝉，而队伍中已经有个法师，你就说别让我用法师，换个吧。我当时可能心情就很不爽了，因为之前打了几局都不好，然后就反驳，最后凶了你一句，然后我们就各删了游戏。呵呵，然后就在微信上聊了起来，你说你哭了，觉得自己很委屈，只不过是玩个游戏，我就凶你了，那以后生活的不得水深火热。我当时心中还很烦躁，我这个人要是心情烦躁起来，就不会讲理了。 聊着聊着，我就问了你一句：“现在后悔还来得及”，其实说完这句话，我很还忐忑，我心中在期待，你肯定的回答，但是没有。但也不意外，毕竟都在气头上，而且还是我的错。 我很害怕失去你，又不想表达出来，想听你心中最真实的想法，我自己心中的想法是“你要是想离开，我不会挽留”，因为你既然都想好了，我不想要个勉强的你。 自我反省话说回来，也该批评自己了，最近是很膨胀啊。毕竟这几年过得都顺风顺水，没什么大的坎坷，可能自己在心态上就有了变化，脾气也见长了，价值观也有了变化，喜欢购物，花钱，穿好看的。没有了高中那种一花钱就想到父母的辛苦。我觉得这样很不好，我要改变自己，虽然之前删游戏是一时之快，但是我想坚持下来，把更多的时间用在学习上。 改进之处 把更多的时间花在学习和陪你上 脾气暴躁需要改改，以后多读书，去图书馆借本自控力的书，每天阅读半个小时 爱说脏话，这个要改，但是没啥很好的方法，就平时多自己注意吧 总结自己写完这些字，心情果然好了点。继续加油，明天会是美好的一天！加油！]]></content>
      <categories>
        <category>心情笔记</category>
      </categories>
      <tags>
        <tag>丹岑</tag>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[iTerm 2 实用快捷键]]></title>
    <url>%2FiTerm%202%20%E5%AE%9E%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[⌘ + Click：可以打开文件，文件夹和链接 ⌘ + n：新建窗口 ⌘ + t：新建标签页 ⌘ + w：关闭当前页 ⌘ + 数字 &amp; ⌘ + 方向键：切换标签页 ⌥⌘ + 数字：切换窗口 ⌘ + enter：切换全屏 ⌘ + d：左右分屏ß ⇧⌘ + d：上下分屏 ⌘ + ;：自动补全历史记录 ⇧⌘ + h：自动补全剪贴板历史 ⌥⌘ + e：查找所有来定位某个标签页 ⌘ + r &amp; ⌃ + l：清屏 ⌘ + /：显示光标位置 ⌥⌘ + b：历史回放 ⌘ + f：查找，然后用 tab 和 ⇧ + tab 可以向右和向左补全，补全之后的内容会被自动复制， 还可以用 ⌥ + enter 将查找结果输入终端 选中即复制，鼠标中键粘贴 很多快捷键都是通用的，和 Emace 等都是一样的 ⌃ + u：清空当前行 ⌃ + a：移动到行首 ⌃ + e：移动到行尾 ⌃ + f：向前移动 ⌃ + b：向后移动 ⌃ + p：上一条命令 ⌃ + n：下一条命令 ⌃ + r：搜索历史命令 ⌃ + y：召回最近用命令删除的文字 ⌃ + h：删除光标之前的字符 ⌃ + d：删除光标所指的字符 ⌃ + w：删除光标之前的单词 ⌃ + k：删除从光标到行尾的内容 ⌃ + t：交换光标和之前的字符]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>iTem2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密钥分发中心（KDC）]]></title>
    <url>%2F%E5%AF%86%E9%92%A5%E5%88%86%E5%8F%91%E4%B8%AD%E5%BF%83%EF%BC%88KDC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简言 密钥分发中心是一种运行在物理安全服务器上的服务，KDC维护着领域中所有安全主体账户信息数据库。 与每一个安全主体的其他信息一起，KDC存储了仅安全主体和KDC知道的加密密钥，这个密钥也称长效密钥（主密钥），用于在安全主体和KDC之间进行交换。 KDC是作为发起方和接收方共同信任的第三方，因为他维护者一个存储着该域中所有账户的账户数据库，也就是说，他知道属于每个账户的名称和派生于该账户密码的Master Key（主密钥）。而用于Alice和Bob相互认证的会话密钥就是由KDC分发的，下面详细讲解KDC分发会话密钥的过程。 分发会话密钥过程1、首先客户端向KDC发送一个会话密钥申请。这个申请的内容可以简单概括为”我是某客户端，我需要个Session Key用于与某服务器通话“。 2、KDC在接收到这个请求的时候，生成一个会话密钥。为了保证这个会话密钥仅仅限于发送请求的客户端和它希望访问的服务器知道，KDC会为这个会话密钥生成两个副本，分别被客户端和服务器使用。然后从账户数据库中提取客户端和服务器的主密钥分别对这两个副本进行对称加密。对于服务器，与会话密钥一起被加密的还包含关于客户端的一些信息，以便对发起连接请求的客户端进行身份认证。 注意：KDC不是直接把这两个会话密钥副本分发客户端和服务器的，因为如果这样做，对于服务器来说会 出现下面两个问题。由于一个服务器会面对若干不同的客户端，而每个客户端都具有一个不同的Session Key。那么服务器就会为所有的客户端维护这样一个会话密钥的列表，这样对服务器来说工作量就非常 大了。由于网络传输的不确定性，可能会出现这样一种情况：客户端很快获得会话密钥用于副本，并将 这个会话密钥作为凭据随同访问请求发送到服务器，但是用于服务器的会话密钥却还没有收到，并且很 有可能这个会话密钥永远也到不了服务器端，这样客户端将永远得不到认证。为了解决这个问题， Kerberos将这两个被加密的副本一同发送给客户端，属于服务器的那份由客户端发送给服务器。因为 这两个会话密钥副本分别是由客户端和服务器端的主密钥加密的，所以不用担心安全问题。 3、通过上面的过程，客户端实际上获得了两组信息：一个是通过自己主密钥加密的会话密钥；另一个是被Server的主密钥加密的数据包，包含会话密钥和关于自己的一些确认信息。在这个基础上，我们再来看看服务器是如何对客户端进行认证的。 4、客户端通过用自己的主密钥对KDC加密的会话密钥进行解密从而获得会话密钥，随后创建认证符（Authenticator，包括客户端信息和时间戳（Timestamp）），并用会话密钥对其加密。最后连同从KDC获得的、被服务器的主密钥加密过的数据包（客户端信息和会话密钥）一并发送到服务器端。我们把通过服务器的主密钥加密过的数据包称为服务票证（Session Ticket）。 5、当服务器接收到这两组数据后，先使用它自己的主密钥对服务票证进行解密，从而获得会话密钥。随后使用该会话密钥解密认证符，通过比较由客户端发送来的认证符中的客户端信息（Client Info）和服务票证中的客户端信息实现对客户端身份的验证。 双方进行了身份认证的同时也获得了会话密钥，那么双方可以进行会话了。 流程图如下： 客户端简称为Alice，服务端简称为Bob 总结参考文章]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>KDC</tag>
        <tag>密钥</tag>
      </tags>
  </entry>
</search>
